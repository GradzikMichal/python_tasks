{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First part of laboratories were conducted by <a href=\"https://github.com/makskliczkowski\">Maksymilian Kliczkowski</a>. He created notebooks we used in laboratories and he help me and my friends with understanding basic and advanced topics in machine learning. I am really glad that I had him as a tutor.\n",
    "\n",
    "In first course of ML/AI we were build ML models from the ground using math.\n",
    "\n",
    "In this notebook I will present only some task we had as a homework from notebook about scikit-learn"
   ],
   "id": "d523ce8f521e9bd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Logistic regression - part taken from older tasks",
   "id": "2bc6bd5ff73cf7b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### k) Create the logistic regression algorithm using the information from above. Implement Newton-Rhapson for it as well.",
   "id": "d2b50571e5df8cab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## OLD CLASSES",
   "id": "6cf65434f0ee5be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Perceptron:\n",
    "    # Initialize the bias and the dimension of the perceptron.\n",
    "    def __init__(self, W : np.array,  b = 0):\n",
    "        self.W = W\n",
    "        self.N = len(W)\n",
    "        self.b = b\n",
    "\n",
    "    ############################## GETTERS ##############################\n",
    "\n",
    "    def get_N(self):\n",
    "        return self.N\n",
    "\n",
    "    def get_b(self):\n",
    "        return self.b\n",
    "\n",
    "    def get_W(self):\n",
    "        return self.W\n",
    "\n",
    "    ############################## SETTERS ##############################\n",
    "\n",
    "    # Create setters for the perceptron\n",
    "    def set_N(self, N):\n",
    "        self.N = N\n",
    "        self.W = np.zeros(N)\n",
    "\n",
    "    def set_b(self, b):\n",
    "        self.b = b\n",
    "\n",
    "    def set_W(self, W : np.array):\n",
    "        if W.ndim != 1:\n",
    "            print(\"Cannot set such weights -> dimension wrong\")\n",
    "            return\n",
    "        self.N = W.shape[0]\n",
    "        self.W = W\n",
    "\n",
    "    ############################## GETTERS OVERRIDE ##############################\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.W[key]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.W[key] = value\n",
    "\n",
    "    def __getslice(self, i, j):\n",
    "        return self.W[i:j]\n",
    "\n",
    "    # set the string output of the perceptron\n",
    "    def __str__(self):\n",
    "        return f\"Am a perceptron of N={self.N} dimension{'s' if self.N > 1 else ''} biased with b={self.b}\"\n",
    "\n",
    "    ############################## OPERATORS OVERRIDE ##############################\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return self.activation_function(other)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    ############################## PERCEPTRON METHODS ##############################\n",
    "\n",
    "    '''\n",
    "    Net output is the basic body action of the perceptron. On top of it, the activation function is used.\n",
    "    '''\n",
    "    def net_output(self, X):\n",
    "        return np.dot(X, self.W) + self.b\n",
    "\n",
    "    '''\n",
    "    Here the activation function is step-like\n",
    "    '''\n",
    "    def activation_function(self, X):\n",
    "        return np.where(self.net_output(X) >= 0.0, 1.0, 0.0).reshape((len(X),1))\n",
    "\n",
    "    '''\n",
    "    Predicts the output of perceptron -> here the class is given\n",
    "    '''\n",
    "    def predict(self, X):\n",
    "        return self.activation_function(X)"
   ],
   "id": "dfeda701558828d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PerceptronLinear(Perceptron):\n",
    "    # Initialize the bias and the dimension of the perceptron -> but not only that\n",
    "    def __init__(self, W : np.array,  b = 0, epo = 100, lr = 0.01):\n",
    "        super().__init__(W, b)\n",
    "        # how many learning iterations we give\n",
    "        self.epo = epo\n",
    "        # what is our step in the gradient\n",
    "        self.lr = lr\n",
    "\n",
    "    ############################## GETTERS ##############################\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.lr\n",
    "\n",
    "    def get_epo(self):\n",
    "        return self.epo\n",
    "\n",
    "    ############################## SETTERS ##############################\n",
    "\n",
    "    def set_lr(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def set_epo(self, epo):\n",
    "        self.epo = epo\n",
    "\n",
    "    ######################## PERCEPTRON METHODS #########################\n",
    "    def activation_function(self, X):\n",
    "        '''\n",
    "        As you can see the activation function has been changed to linear regression. What does reshape do?\n",
    "        After you figure it out, note that it is the usual way of having the output of the machine learning algorithm. Why?\n",
    "        '''\n",
    "        return (self.net_output(X)).reshape(-1,1)\n",
    "\n",
    "    def loss(self, y_true : np.array, y_pred : np.array):\n",
    "        '''\n",
    "        Loss function for the perceptron -> here we use the Mean Square Error\n",
    "        '''\n",
    "        square = np.square(y_true - y_pred)\n",
    "        return np.mean(square)\n",
    "\n",
    "    def gradient(self, x_true, y_true, prediction):\n",
    "        '''\n",
    "        Single step of the gradient, here it is calculated analytically (linear regression)\n",
    "        '''\n",
    "        delta_i = (y_true.flatten() - prediction)\n",
    "        suma_w  = np.multiply(delta_i[:, np.newaxis], x_true)\n",
    "        suma_b  = delta_i\n",
    "        return suma_b, suma_w\n",
    "\n",
    "    def fit(self, X, y, randomstate = None, batch = 1, verbose = False):\n",
    "        '''\n",
    "        Fit function allows to obtain the (probably most?) correct weights for the perceptron via the gradient descent algorithm.\n",
    "        '''\n",
    "\n",
    "        if type(X) != np.ndarray:\n",
    "            X = np.array(X)\n",
    "        if type(y) != np.ndarray:\n",
    "            y = np.array(y).reshape(-1,1)\n",
    "\n",
    "        # give fit the parameter randomstate and whenever it is not None, the weights\n",
    "        # are reset to be random normal - this ensures random starting point of gradient descent\n",
    "        if randomstate is not None:\n",
    "            self.W = np.random.normal(0.0, 0.1, self.N)\n",
    "            self.b = np.random.normal(0.0, 1.0)\n",
    "\n",
    "        # Save the history of the losses. Why?\n",
    "        history = []\n",
    "        # If we want to calculate the gradient in buckets (look for description of the batch)\n",
    "        bucket_num = len(X) // batch\n",
    "        # slice the data onto batches without shuffling (no stochasticity)\n",
    "        slicing = lambda x, b: x[(b-1)*batch:b*batch]\n",
    "\n",
    "        # iterate epochs\n",
    "        for epo in range(self.epo):\n",
    "            # iterate batches\n",
    "            loss = 0.0\n",
    "            for bin in range(1, bucket_num + 1):\n",
    "                X_slice = slicing(X,bin)\n",
    "                y_slice = slicing(y,bin)\n",
    "                # predict the output for a given slice (what is the shape of the output?)\n",
    "                pred = self.predict(X_slice)\n",
    "\n",
    "                suma_b, suma_w = self.gradient(X_slice, y_slice, pred.flatten())\n",
    "\n",
    "                # calculate loss\n",
    "                loss += self.loss(y_slice, pred.flatten())\n",
    "\n",
    "                # update the weights\n",
    "                self.W += np.mean(suma_w, axis = 0) * self.lr\n",
    "                self.b += np.mean(suma_b, axis = 0) * self.lr\n",
    "            # calculate average loss\n",
    "            loss/=bucket_num\n",
    "            if verbose:\n",
    "                print(f'epo:{epo}->loss={loss}')\n",
    "            history.append(loss.flatten())\n",
    "        return np.array(history).flatten()\n",
    "\n",
    "    def plot_history(self, history, ax = None):\n",
    "        '''\n",
    "        Basic history plot\n",
    "        '''\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "        ax.set_xlabel('epo')\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.plot(history)"
   ],
   "id": "ec539666cb80c688"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PerceptronBinary(PerceptronLinear):\n",
    "    def __init__(self, W : np.array,  b = 0, epo = 100, lr = 0.01):\n",
    "        super().__init__(W, b, epo, lr)\n",
    "\n",
    "    def activation_function(self, X):\n",
    "        '''\n",
    "        $\\Phi(x) = sign(x)\n",
    "        '''\n",
    "        return np.where(self.net_output(X) >= 0.0, 1.0, -1.0).reshape(-1,1)\n",
    "\n",
    "    def loss(self, y_true : np.array, y_pred : np.array):\n",
    "        '''\n",
    "        Loss function for the perceptron -> here we use knowledge that the classes can be either {-1, 1}\n",
    "        '''\n",
    "        square = np.square(1.0-y_true * y_pred)\n",
    "        return np.mean(square)\n",
    "\n",
    "    def gradient(self, x_true, y_true, prediction):\n",
    "        '''\n",
    "        Single step of the gradient, here it is calculatable analytically (linear regression)\n",
    "        '''\n",
    "        val = np.multiply(y_true, (1.0-y_true * prediction))\n",
    "        suma_w = np.multiply(val, x_true)\n",
    "        suma_b = val\n",
    "        return suma_b, suma_w"
   ],
   "id": "dcaee36fe603c8d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PerceptronLogistic(PerceptronBinary):\n",
    "    def __init__(self, W : np.array,  b = 0, epo = 100, lr = 0.01):\n",
    "        super().__init__(W, b, epo, lr)\n",
    "\n",
    "    def activation_function(self, X):\n",
    "        return sc.special.expit(X*self.W).shape(-1,1)\n",
    "\n",
    "    def loss(self, y_true : np.array, y_pred : np.array):\n",
    "        lg = np.log(1 - y_pred)\n",
    "        mul = np.multiply((1-y_true), lg)\n",
    "        lh = np.multiply(y_true, np.log(y_pred))\n",
    "        return lh + mul\n",
    "\n",
    "    def gradient(self, x_true, y_true, prediction):\n",
    "      sig = self.activation_function(np.multiply(self.W * x_true))\n",
    "      return np.multiply((y_true - sig), x_true)"
   ],
   "id": "d2c647e31a9ee33b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# data\n",
    "X = (np.concatenate((np.random.uniform(low=-2.0, high=0.4, size=50), np.random.uniform(low=-0.4, high=3.0, size=50)))).reshape(-1,1)\n",
    "Y = np.array([0]*50 + [1]*50).reshape(-1,1)\n",
    "\n",
    "# fit the perceptron\n",
    "p=PerceptronLogistic([1],0.1,500,0.01)\n",
    "history=p.fit(X, Y)\n",
    "p.plot_history(history)\n",
    "Y_pred = p.predict(X)\n",
    "plt.show()\n",
    "\n",
    "# plotting data and prediction\n",
    "plt.scatter(X,Y_pred)\n",
    "plt.scatter(X,Y,c='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('class')\n",
    "plt.show()"
   ],
   "id": "3e6d5fa7fe3c07eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### a) Use the data below to test the final logistic regression. Do you see the probability corespondence?",
   "id": "6dd702588df6d6c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# data (create the X variables from -2.0 to 0.4 randomly) and concatenating it with slightly different data, make this nonseparable linearly by creating an overlap\n",
    "X = (np.concatenate((np.random.uniform(low=-2.0, high=0.4, size=50), np.random.uniform(low=-0.4, high=3.0, size=50)))).reshape(-1,1)\n",
    "Y = np.array([0]*50 + [1]*50).reshape(-1,1)"
   ],
   "id": "2cce23181a928ca6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# fit the perceptron\n",
    "p       = PerceptronLogistic(np.array(np.random.random(1)))\n",
    "history = p.fit(X, Y)\n",
    "p.plot_history(history)\n",
    "Y_pred  = p.activation_function(X)\n",
    "plt.show()\n",
    "\n",
    "# plotting data and prediction\n",
    "plt.scatter(X,Y_pred)\n",
    "plt.scatter(X,Y,c='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('class')\n",
    "plt.show()"
   ],
   "id": "f8e45f8f4e4df3aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### b) Implement a correct Hessian using the templates from the previous exercises.",
   "id": "f073da8710c1dfe6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PerceptronLogistic_v2(PerceptronBinary):\n",
    "    def __init__(self, W : np.array,  b = 0, epo = 100, lr = 0.01):\n",
    "        super().__init__(W, b, epo, lr)\n",
    "\n",
    "    def activation_function(self, X):\n",
    "        '''\n",
    "        Sigmoid - plot it if you want to\n",
    "        '''\n",
    "        return (1.0/(1.0+np.exp(-self.net_output(X)))).reshape(-1,1)\n",
    "\n",
    "    def loss(self, y_true : np.array, y_pred : np.array):\n",
    "        '''\n",
    "        Maximal likelyhood loss\n",
    "        '''\n",
    "        return -np.sum(np.multiply(y_true, np.log(y_pred)) + np.multiply((1.0-y_true), np.log(1.0-y_pred)))\n",
    "\n",
    "    def gradient(self, x_true, y_true, prediction):\n",
    "        '''\n",
    "        Is of the same character as linear regression\n",
    "        '''\n",
    "        delta_i = (y_true.flatten() - prediction)\n",
    "        suma_w  = np.multiply(delta_i[:, np.newaxis], x_true)\n",
    "        suma_b  = delta_i\n",
    "        return suma_b, suma_w\n",
    "\n",
    "    def hessian(self, x_true, y_true, y_pred):\n",
    "        '''\n",
    "        Calculates the hessian using logistic regression sigmoid activation\n",
    "        - x_true : input of the model (for convinience in flattened version)\n",
    "        - y_true : supervised learning true classes\n",
    "        - y_pred : predictions of the model for a given batch\n",
    "        '''\n",
    "        hess = np.zeros((len(x_true)+1, len(x_true)+1))\n",
    "\n",
    "        print(x_true, y_true, y_pred)\n",
    "        # we add 1 because of the bias (go through rows)\n",
    "        for i in range(len(x_true) + 1):\n",
    "            # taking bias into account\n",
    "            for j in range(len(x_true) + 1):\n",
    "                x1 = 1 if i == 0 else x_true[i-1]\n",
    "                x2 = 1 if j == 0 else x_true[j-1]\n",
    "                hess[i][j] = np.multiply(y_pred, (1 - y_pred)) * x1 * x2\n",
    "        return hess\n",
    "\n",
    "    def newton_rap(self, X, y, randomstate = None, verbose = False):\n",
    "        '''\n",
    "        Newton rap : https://www.youtube.com/watch?v=8yis7GzlXNM\n",
    "        '''\n",
    "        if type(X) != np.ndarray:\n",
    "            X = np.array(X)\n",
    "        if type(y) != np.ndarray:\n",
    "            y = np.array(y).reshape(-1,1)\n",
    "\n",
    "        # give fit the parameter randomstate and whenever it is not None, the weights\n",
    "        # are reset to be random normal - this ensures random starting point of gradient descent\n",
    "        if randomstate is not None:\n",
    "            self.W = np.random.normal(0.0, 0.1, self.N)\n",
    "            self.b = np.random.normal(0.0, 1.0)\n",
    "\n",
    "        # Save the history of the losses. Why?\n",
    "        history = []\n",
    "\n",
    "        # iterate epochs\n",
    "        for epo in range(self.epo):\n",
    "            # iterate batches\n",
    "            loss = 0.0\n",
    "            for i, X_slice in enumerate(X):\n",
    "                y_slice = y[i]\n",
    "                # predict the output for a given slice (what is the shape of the output?)\n",
    "                pred    = self.predict(X_slice)\n",
    "                suma_b, suma_w = self.gradient(X_slice, y_slice, pred.flatten())\n",
    "                # calculate the gradient\n",
    "                grad    = np.array(list(suma_b.flatten()) + list(suma_w.flatten()))\n",
    "                # calculate the hessian\n",
    "                hessian = self.hessian(X_slice, y_slice, pred.flatten())\n",
    "                # calculate the update vector (use pseudoinverse to be numerically safe)\n",
    "                update  = np.linalg.pinv(hessian).dot(grad)\n",
    "                # update weights\n",
    "                self.W  += np.array(self.lr*update[1:])\n",
    "                self.b  += np.array(self.lr*update[0])\n",
    "                # calculate loss\n",
    "                loss    += self.loss(y_slice, pred.flatten())\n",
    "            if verbose:\n",
    "                print(f'epo:{epo}->loss={loss}')\n",
    "            history.append(loss.flatten())\n",
    "        return np.array(history).flatten()"
   ],
   "id": "c7d660057920b827"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# data\n",
    "X   =   (np.concatenate((np.random.uniform(low=-2.0, high=0.4, size=50), np.random.uniform(low=-0.4, high=3.0, size=50)))).reshape(-1,1)\n",
    "Y   =   np.array([0]*50 + [1]*50).reshape(-1,1)\n",
    "# fit the perceptron\n",
    "p       =   PerceptronLogistic_v2([1], 0.1, 400, lr = 2e-4)\n",
    "history =   p.newton_rap(X, Y)\n",
    "p.plot_history(history)\n",
    "Y_pred  =   p.predict(X)\n",
    "plt.show()\n",
    "\n",
    "# plotting data and prediction\n",
    "plt.scatter(X,Y_pred)\n",
    "plt.scatter(X,Y,c='red')\n",
    "plt.axhline(0.5)\n",
    "plt.axvline(np.min(X[Y==1]))\n",
    "plt.axvline(np.max(X[Y==0]))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('class')\n",
    "plt.show()"
   ],
   "id": "31784ed2623d2703"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
