{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First part of laboratories were conducted by <a href=\"https://github.com/makskliczkowski\">Maksymilian Kliczkowski</a>. He created notebooks we used in laboratories and he help me and my friends with understanding basic and advanced topics in machine learning. I am really glad that I had him as a tutor.\n",
    "\n",
    "In first course of ML/AI we were build ML models from the ground using math.\n",
    "\n",
    "In this notebook I will present only some task we had as a homework from notebook about scikit-learn"
   ],
   "id": "8faf3c10ae507319"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import scipy as sc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "176d300d47016618"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# <center>Support Vector Machines</center>\n",
   "id": "c330e050a6d3c6d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### a) Create a random two-dimensional weights vector and plot the decision line for different biases.\n",
    "\n",
    "Partially Maks's code"
   ],
   "id": "d9a77e15f5929e2f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "# set all the biases\n",
    "b   = np.random.random() / 10\n",
    "b1  = b + 1\n",
    "b2  = b - 1\n",
    "# set the random weights vector\n",
    "W   = np.random.random(2)\n",
    "\n",
    "# define the calculated points on which the decision line crosses the y-axis\n",
    "c   = -b/W[1]\n",
    "c1  = -b1/W[1]\n",
    "c2  = -b2/W[1]\n",
    "# slope of the line\n",
    "a   = -W[0]/W[1]\n",
    "\n",
    "# perpendicular function to W\n",
    "fp  =   lambda x: a * x + c\n",
    "fp1 =   lambda x: a * x + c1\n",
    "fp2 =   lambda x: a * x + c2\n",
    "\n",
    "# a lambda for parallel function to W (contains the vector W), let's say it crosses the y axis at the same point\n",
    "f   =   lambda x: -1.0/a * x + c\n",
    "\n",
    "# set xrange\n",
    "x   =   np.arange(-2.5, 2.5, 0.11)\n",
    "\n",
    "# plot decision functions\n",
    "ax.plot(x,fp(x),\n",
    "         color = 'green', alpha = 0.3, linewidth=7,\n",
    "         label = f'$f(x)$ decision line for $b={b:.3f}$')\n",
    "ax.plot(x,fp1(x), linestyle = '-',\n",
    "         color = 'red', alpha = 0.3, linewidth=3,\n",
    "         label = f'$f(x)$ lower width of the street')\n",
    "ax.fill_between(x, -2, fp1(x), color = 'red', alpha = 0.2)\n",
    "ax.plot(x,fp2(x),linestyle = '-',\n",
    "         color = 'blue', alpha = 0.3, linewidth=3,\n",
    "         label = f'$f(x)$ upper width of the street')\n",
    "ax.fill_between(x, fp2(x), 2, color = 'blue', alpha = 0.2)\n",
    "# plot the weights function and the vector\n",
    "ax.plot(x,f(x),\n",
    "         color = 'black', alpha = 0.3, linewidth=7,\n",
    "         label = r'$\\vec{W}$ ' + f'for $b={b:.3f}$')\n",
    "\n",
    "# plot labels\n",
    "ax.text(W[0] + 0.05, W[1] + 0.05 + c, r'$\\vec{W}$')\n",
    "ax.text(0 + 0.05, c-0.05, f\"$c={c:.3f}$\")\n",
    "\n",
    "# plot W vector\n",
    "ax.arrow(0, c, W[0], W[1], head_width = 0.1, width = 0.05,\n",
    "          head_length=0.1, color = 'blue')\n",
    "ax.scatter(0, c, color = 'red')\n",
    "\n",
    "# check condition W * X laying on f(x) + b = 0\n",
    "xp = -0.8\n",
    "yp = fp(xp)\n",
    "ax.scatter(xp,yp)\n",
    "ax.text(xp+0.05, yp+0.05, r'$\\vec{W}\\cdot \\vec{X} + b=$' + f'{xp*W[0] + yp*W[1] + b : .3f}')\n",
    "\n",
    "# axes\n",
    "ax.axhline(0)\n",
    "ax.axvline(0)\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-2, 2)\n",
    "plt.legend()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### b) Take the data below and analytically find the solution of the Lagrange problem.\n",
    "- Plot the data and plot the corresponding support vectors.\n",
    "- Plot the weights vector and the street.\n",
    "- Calculate the width of the street.\n",
    "- Experiment with adding a third point to the data.\n",
    "- You may use the previous functions"
   ],
   "id": "2d75280f260354ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_1 = np.array([1,1])\n",
    "Y_1 = -1\n",
    "X_2 = np.array([-1,-1])\n",
    "Y_2 = 1\n",
    "plt.scatter(X_1[0], X_1[1], label = Y_1)\n",
    "plt.scatter(X_2[0], X_2[1], label = Y_2)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.axvline(0)\n",
    "plt.axhline(0)\n",
    "plt.legend()"
   ],
   "id": "5f5021bbb68c2002"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# W = \\sum _i a_i y_i X_i\n",
    "a_1     = 1\n",
    "a_2     = 1\n",
    "W       = X_1 * Y_1 * a_1 + X_2 * Y_2 * a_2\n",
    "print(f'W={W}')\n",
    "norm_w  = np.sqrt(np.sum(np.square(W)))\n",
    "print(f'norm={norm_w}')\n",
    "b       = Y_1 - W.dot(X_1)\n",
    "print(f'b={b}')\n",
    "width   = (X_2 - X_1).dot(W/norm_w)\n",
    "print(f'width={width}', r'width/$2\\sqrt{2}=$' + f'{width / (2*np.sqrt(2))}')\n"
   ],
   "id": "e5675e2a6e38d790"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax     = plt.subplots(figsize=(8,8))\n",
    "\n",
    "# define the calculated points\n",
    "c           = -b/W[1]\n",
    "a           = -W[0]/W[1]\n",
    "print(c,a)\n",
    "\n",
    "# perpendicular function to W\n",
    "fp          = lambda x: a * x + c\n",
    "fp1         = lambda x: a * (x-X_2[0]) + c + X_2[1]\n",
    "fp2         = lambda x: a * (x-X_1[0]) + c + X_1[1]\n",
    "\n",
    "# a function for parallel function to W (contains the vector W)\n",
    "f           = lambda x: -1.0/a * x + c\n",
    "\n",
    "# set xrange\n",
    "x           = np.arange(-5, 5, 0.11)\n",
    "\n",
    "# plot decision function\n",
    "ax.plot(x,fp(x),\n",
    "         color = 'green', alpha = 0.3, linewidth=7,\n",
    "         label = f'$f(x)$ decision line for $b=$'+str(b))\n",
    "ax.plot(x,fp1(x), linestyle = ':',\n",
    "         color = 'blue', alpha = 0.3, linewidth=3,\n",
    "         label = f'$f(x)$ lower width of the street')\n",
    "ax.fill_between(x, x.min(), fp1(x), color = 'blue', alpha = 0.2)\n",
    "ax.plot(x,fp2(x),linestyle = ':',\n",
    "         color = 'red', alpha = 0.3, linewidth=3,\n",
    "         label = f'$f(x)$ upper width of the street')\n",
    "ax.fill_between(x, fp2(x), x.max(), color = 'red', alpha = 0.2)\n",
    "\n",
    "# plot the weights function\n",
    "ax.plot(x,f(x),\n",
    "         color = 'black', alpha = 0.3, linewidth=7,\n",
    "         label = r'$\\vec{W}$ for $b=$'+str(b))\n",
    "\n",
    "\n",
    "# plot W vector\n",
    "norm = np.sqrt(np.sum(np.square(W)))\n",
    "ax.arrow(0, 0, W[0]/norm, W[1]/norm, head_width = 0.1, width = 0.02,\n",
    "          head_length=0.1, color = 'black')\n",
    "# plot labels\n",
    "ax.text(W[0]/norm + 0.1, W[1]/norm - 0.1, r'$\\vec{W}$', fontsize=20)\n",
    "\n",
    "# to plot width\n",
    "x_width = -1.0\n",
    "y_width = fp(x_width)\n",
    "dx_width = X_2[0]\n",
    "dy_width = X_2[1]\n",
    "ax.arrow(x_width, y_width, dx_width, dy_width, head_width = 0.1,\n",
    "          head_length=0.1, color = 'black')\n",
    "dx_width = X_1[0]\n",
    "dy_width = X_1[1]\n",
    "ax.arrow(x_width, y_width, dx_width, dy_width, head_width = 0.1,\n",
    "          head_length=0.1, color = 'black')\n",
    "ax.text(x_width+ 0.1, y_width - 0.1,r'$2\\sqrt {2}$', fontsize=20)\n",
    "\n",
    "\n",
    "# check condition W * X laying on f(x) + b = 0\n",
    "xp1 = X_1[0]\n",
    "xp2 = X_2[0]\n",
    "yp1 = X_1[1]\n",
    "yp2 = X_2[1]\n",
    "ax.scatter(xp1,yp1, color = 'red')\n",
    "ax.scatter(xp2,yp2, color = 'blue')\n",
    "\n",
    "# plot the support vectors\n",
    "ax.arrow(0, 0, xp1, yp1, head_width = 0.1, width = 0.02,\n",
    "          head_length=0.1, color = 'red')\n",
    "ax.text(xp1 + 0.1, yp1 - 0.1, r'$\\vec{X_+}$', fontsize=20)\n",
    "ax.arrow(0, 0, xp2, yp2, head_width = 0.1, width = 0.02,\n",
    "          head_length=0.1, color = 'blue')\n",
    "ax.text(xp2 + 0.1, yp2 - 0.1, r'$\\vec{X_-}$', fontsize=20)\n",
    "\n",
    "\n",
    "# axes\n",
    "ax.axhline(0)\n",
    "ax.axvline(0)\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_xlim(-4.5, 4.5)\n",
    "ax.set_ylim(-4.5, 4.5)\n",
    "plt.legend()"
   ],
   "id": "d5cee294a34993b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### c) Follow the numerical algorithm to maximize the Lagrangian function.\n",
    "You can use the scipy library, which implements [optimization minimizer](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html). Define Lagrange function to be $-L(\\vec{W})$ in order to find the maximum."
   ],
   "id": "e7fec47ca3332f77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Lagrange function definition",
   "id": "502a8f566e3b12aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "Note that the Lagrange function has minus at the beginning in order to use the scipy.optimize.minimize function.\n",
    "'''\n",
    "def Lagrange(alphas : np.ndarray, Y : np.ndarray, X : np.ndarray):\n",
    "  alph_sum = sum(alphas)\n",
    "  RHS = 0\n",
    "  for i in range(len(Y)):\n",
    "    for j in range(len(Y)):\n",
    "      RHS += alphas[i] * alphas[j] * Y[i] * Y[j] * np.dot(X[i], X[j])\n",
    "\n",
    "  Lag = alph_sum - 1/2 * RHS\n",
    "  return -Lag"
   ],
   "id": "b8471428bba8b504"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scipy import optimize\n",
    "\n",
    "# concatenate the vectors and outputs\n",
    "X = np.array([X_1, X_2])\n",
    "Y = np.array([Y_1, Y_2])\n",
    "\n",
    "# plot the corresponding Lagrange function as an image to find the minimum visualy\n",
    "a_test = np.arange(-2, 2, 0.1)\n",
    "data = np.zeros((len(a_test), len(a_test), 1))\n",
    "for i,vi in enumerate(a_test):\n",
    "    for j,vj in enumerate(a_test):\n",
    "        data[i,j] = Lagrange([vi,vj], Y, X)\n",
    "\n",
    "c = plt.imshow(data)\n",
    "plt.colorbar(c)\n",
    "plt.ylabel(r'j for $\\alpha[j]$')\n",
    "plt.xlabel(r'i for $\\alpha[i]$')\n",
    "plt.title(r\"Value of Lagrange function for different $\\alpha$.\")"
   ],
   "id": "245066f6727bf61b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Define the constraint with [scipy.optimize.LinearConstraint](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.LinearConstraint.html#scipy.optimize.LinearConstraint)",
   "id": "467ac16b9777f3c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define the constraint\n",
    "constraint = optimize.LinearConstraint(Y, 0.0, 0.0)\n",
    "print(constraint)"
   ],
   "id": "3bb93da25332c4ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Optimize and get the values of $\\vec{\\alpha}$",
   "id": "e662a3abe95489e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(Lagrange([1., 1.], Y,X))\n",
    "x = optimize.minimize(Lagrange, x0 = (1,1), args = (Y, X), constraints=constraint)\n",
    "alphas = x['x']\n",
    "alphas"
   ],
   "id": "44ce6bea16d1d555"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Get $\\vec{W}$.",
   "id": "4d20b098b9cb0fe9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "W = alphas[0] * X[0] * Y[0] + alphas[1] * X[1] * Y[1]\n",
    "W"
   ],
   "id": "3b35cbc91a73adc3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Get the bias $b$.",
   "id": "e6d9bd2b3b3b2a71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "b = Y_1 - W.dot(X_1)\n",
    "b"
   ],
   "id": "59cc5a359cfa6828"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot it!",
   "id": "846c7443dea51812"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax     = plt.subplots(figsize=(8,8))\n",
    "\n",
    "# define the calculated points\n",
    "c           = -b/W[1]\n",
    "a           = -W[0]/W[1]\n",
    "print(c,a)\n",
    "\n",
    "# perpendicular function to W\n",
    "fp          = lambda x: a * x + c\n",
    "fp1         = lambda x: a * (x-X_2[0]) + c + X_2[1]\n",
    "fp2         = lambda x: a * (x-X_1[0]) + c + X_1[1]\n",
    "\n",
    "# a function for parallel function to W (contains the vector W)\n",
    "f           = lambda x: -1.0/a * x + c\n",
    "\n",
    "# set xrange\n",
    "x           = np.arange(-5, 5, 0.11)\n",
    "\n",
    "# plot decision function\n",
    "ax.plot(x,fp(x),\n",
    "         color = 'green', alpha = 0.3, linewidth=7,\n",
    "         label = f'$f(x)$ decision line for $b=$'+str(b))\n",
    "ax.plot(x,fp1(x), linestyle = ':',\n",
    "         color = 'blue', alpha = 0.3, linewidth=3,\n",
    "         label = f'$f(x)$ lower width of the street')\n",
    "ax.fill_between(x, x.min(), fp1(x), color = 'blue', alpha = 0.2)\n",
    "ax.plot(x,fp2(x),linestyle = ':',\n",
    "         color = 'red', alpha = 0.3, linewidth=3,\n",
    "         label = f'$f(x)$ upper width of the street')\n",
    "ax.fill_between(x, fp2(x), x.max(), color = 'red', alpha = 0.2)\n",
    "\n",
    "# plot the weights function\n",
    "ax.plot(x,f(x),\n",
    "         color = 'black', alpha = 0.3, linewidth=7,\n",
    "         label = r'$\\vec{W}$ for $b=$'+str(b))\n",
    "\n",
    "\n",
    "# plot W vector\n",
    "norm = np.sqrt(np.sum(np.square(W)))\n",
    "ax.arrow(0, 0, W[0]/norm, W[1]/norm, head_width = 0.1, width = 0.02,\n",
    "          head_length=0.1, color = 'black')\n",
    "# plot labels\n",
    "ax.text(W[0]/norm + 0.1, W[1]/norm - 0.1, r'$\\vec{W}$', fontsize=20)\n",
    "\n",
    "# to plot width\n",
    "x_width = -1.0\n",
    "y_width = fp(x_width)\n",
    "dx_width = X_2[0]\n",
    "dy_width = X_2[1]\n",
    "ax.arrow(x_width, y_width, dx_width, dy_width, head_width = 0.1,\n",
    "          head_length=0.1, color = 'black')\n",
    "dx_width = X_1[0]\n",
    "dy_width = X_1[1]\n",
    "ax.arrow(x_width, y_width, dx_width, dy_width, head_width = 0.1,\n",
    "          head_length=0.1, color = 'black')\n",
    "ax.text(x_width+ 0.1, y_width - 0.1,r'$2\\sqrt {2}$', fontsize=20)\n",
    "\n",
    "\n",
    "# check condition W * X laying on f(x) + b = 0\n",
    "xp1 = X_1[0]\n",
    "xp2 = X_2[0]\n",
    "yp1 = X_1[1]\n",
    "yp2 = X_2[1]\n",
    "ax.scatter(xp1,yp1, color = 'red')\n",
    "ax.scatter(xp2,yp2, color = 'blue')\n",
    "\n",
    "# plot the support vectors\n",
    "ax.arrow(0, 0, xp1, yp1, head_width = 0.1, width = 0.02,\n",
    "          head_length=0.1, color = 'red')\n",
    "ax.text(xp1 + 0.1, yp1 - 0.1, r'$\\vec{X_+}$', fontsize=20)\n",
    "ax.arrow(0, 0, xp2, yp2, head_width = 0.1, width = 0.02,\n",
    "          head_length=0.1, color = 'blue')\n",
    "ax.text(xp2 + 0.1, yp2 - 0.1, r'$\\vec{X_-}$', fontsize=20)\n",
    "\n",
    "\n",
    "# axes\n",
    "ax.axhline(0)\n",
    "ax.axvline(0)\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_xlim(-4.5, 4.5)\n",
    "ax.set_ylim(-4.5, 4.5)\n",
    "plt.legend()"
   ],
   "id": "f7bc00c5beb27a30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Another labs",
   "id": "130d920ae0681d5b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### b) Take the provided data and compute the solution to the Lagrange problem through numerical methods.\n",
    "- Plot the data and plot the corresponding support vectors.\n",
    "- Plot the weights vector and the street by filling the space between the support lines. Use gradient to mark the strength of the slack variables.\n",
    "- Calculate the width of the street.\n",
    "- Find the slack variables for the outliers. Plot the distribution of the slack variables.\n",
    "You can use the optimize method from Scipy: [optimization minimizer](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) as before. Define Lagrange function to be $-L(\\vec{W})$ in order to find the maximum and use the constraint on the `width of the street` to find the optimal values. Experiment with $C$ parameter."
   ],
   "id": "3459d4af74ade1ee"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy import optimize\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.xlim(-1.0, 1.0)\n",
    "plt.ylim(-1.0, 1.0)\n",
    "\n",
    "# create linearly separable data\n",
    "n_samples   =   25\n",
    "# create random points from that are defined as y > x to be assign with value -1\n",
    "X1 = [-1.0 + 2*np.random.random() for i in range(n_samples)]\n",
    "X1 = [np.array([i, i + np.random.random()]) for i in X1]\n",
    "Y1 = [-1 for i in range(len(X1))]\n",
    "# create random points from that are defined as y < x to be assign with value 1\n",
    "X2 = [-1.0 + 2*np.random.random() for i in range(n_samples)]\n",
    "X2 = [np.array([i, i - np.random.random()]) for i in X2]\n",
    "Y2 = [1 for i in range(len(X2))]\n",
    "\n",
    "# add some outliers (the points y > x have the value 1 and y < x -1)\n",
    "n_outliers = 4\n",
    "X1_o = [-1.0 + 2*np.random.random() for i in range(n_outliers)]\n",
    "X1_o = [np.array([i, i - np.random.random()]) for i in X1_o]\n",
    "Y1_o = [-1 for i in range(len(X1_o))]\n",
    "X2_o = [-1.0 + 2*np.random.random() for i in range(n_outliers)]\n",
    "X2_o = [np.array([i, i + np.random.random()]) for i in X2_o]\n",
    "Y2_o = [-1 for i in range(len(X2_o))]\n",
    "\n",
    "# plot the values\n",
    "plt.plot(np.arange(-1.0, 1.0, 1e-3), np.arange(-1.0, 1.0, 1e-3), linestyle = '--')\n",
    "plt.scatter(np.array(X1)[:,0], np.array(X1)[:,1],       color = 'red')\n",
    "plt.scatter(np.array(X1_o)[:,0], np.array(X1_o)[:,1],   marker = 'x', color = 'red')\n",
    "plt.scatter(np.array(X2)[:,0], np.array(X2)[:,1],       color = 'blue')\n",
    "plt.scatter(np.array(X2_o)[:,0], np.array(X2_o)[:,1],   marker = 'x', color = 'blue')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "# concatenate all the data\n",
    "X_train = np.array(X1 + X2 + X1_o + X2_o)\n",
    "Y_train = np.array(Y1 + Y2 + Y1_o + Y2_o)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.2)"
   ],
   "id": "31d8417269545703",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "Note that the Lagrange function has minus at the beginning in order to use the scipy.optimize.minimize function.\n",
    "'''\n",
    "Y_Y_T   = np.zeros((len(Y_train), len(Y_train)))\n",
    "DOTS    = np.zeros((len(Y_train), len(Y_train)))\n",
    "\n",
    "# fill the values that we can calculate beforehand. Notice that those values include the dot products.\n",
    "# Why is that so? Try answering the question taking into account the information from the lecture.\n",
    "# Try to implemet this to be quicker, avoiding the for loops!\n",
    "\n",
    "for i,xi in enumerate(X_train):\n",
    "    for j,xj in enumerate(X_train):\n",
    "        DOTS[i,j]   = np.dot(xi, xj)\n",
    "        Y_Y_T[i,j]  = Y_train[i] * Y_train[j]\n",
    "#Y_Y_T = np.outer(Y_train, Y_train))\n",
    "#DOTS = np.dot(X_train, X_train.T)\n",
    "\n",
    "print(DOTS[0])\n",
    "print(X_train[0])\n",
    "print(np.dot(X_train, X_train.T)[0])\n",
    "print(DOTS == np.dot(X_train, X_train.T))\n",
    "def Lagrange(alphas : np.ndarray):\n",
    "    sum_alph = sum(alphas)\n",
    "    Lag = sum_alph - (1/2) * np.sum(np.outer(alphas.T, alphas) * Y_Y_T * DOTS)\n",
    "    return -Lag\n",
    "    # it's a me, Lagrange function!\n",
    "print(Lagrange(np.ones(len(X_train))))"
   ],
   "id": "b71f306f30263456"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Define the constraint with [scipy.optimize.LinearConstraint](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.LinearConstraint.html#scipy.optimize.NonLinearConstraint).\n",
    "How many constraints should we introduce?"
   ],
   "id": "ebe2803bfc1a83bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# define the constraint\n",
    "C           = 1.0\n",
    "# y_train * alphas = 0\n",
    "constraint  = optimize.LinearConstraint(Y_train, 0.0, 0.0)\n",
    "constraints = []\n",
    "# how many constraints should there be?\n",
    "for i in range(len(Y_train)):\n",
    "    A = np.zeros(len(Y_train))\n",
    "    A[i] = 1\n",
    "    cons = optimize.LinearConstraint(A, 0.0, C)\n",
    "    # .\n",
    "    constraints.append(cons)\n"
   ],
   "id": "2407104c61d356"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Optimize and get the values of $\\vec{\\alpha}$",
   "id": "d0a236626d809f93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "a       = optimize.minimize(Lagrange,\n",
    "                            x0 = 0.05 * np.random.random(len(Y_train)),\n",
    "                            constraints=[constraint] + constraints)\n",
    "alphas  = a['x']\n",
    "alphas"
   ],
   "id": "f1c00933a03ae7ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Determine the value of $b$ corresponding to any of the Support Vectors $\\vec{X}_i$. To identify support vectors, locate $\\alpha _i$ values that are non-zero.",
   "id": "e3b6199f788aa339"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# find support vectors by identifying nonzero alphas\n",
    "support_idx = np.where(alphas > 0.99)\n",
    "\n",
    "supports    =   []\n",
    "for i in range(len(alphas)):\n",
    "    if -1e-10<alphas[i] >1e-10:\n",
    "      supports.append((alphas[i], Y_train[i], X_train[i]))\n",
    "\n",
    "print(supports)"
   ],
   "id": "aa85d250628b5706"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Find the corresponding value for $\\vec{W}$ and $b$.",
   "id": "65ef073eefeb95b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "W   = np.zeros((2))\n",
    "# try to avoid using this for loop\n",
    "#for i in range(len(alphas)):\n",
    "#     W += alphas[i] * Y_train[i] * X_train[i]\n",
    "\n",
    "W = np.dot(alphas*Y_train, X_train)\n",
    "print(\"W =\", W)\n",
    "\n",
    "\n",
    "b = np.zeros(len(supports))\n",
    "for i, (alpha, y, x) in enumerate(supports):\n",
    "    # find b for any of the support vectors\n",
    "    b[i] = y - np.dot(W, x)\n",
    "\n",
    "#for i, (alpha, y, x) in enumerate(supports):\n",
    "    # find b for any of the support vectors\n",
    "#  W\n",
    "print(b)\n",
    "print(Y_train - np.dot(X_train, W))"
   ],
   "id": "f98beb0589f520bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Find the slack variables as $\\xi _i = \\max (0, 1 - (\\vec{W} \\cdot \\vec{x} _ i + b) y_i)$",
   "id": "f068374279e6e5f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "slackVars = []\n",
    "for i, x in enumerate(X_train):\n",
    "    slackVars.append(np.max((0, 1 - (np.dot(W, x) + b[2])* Y_train[i])))\n",
    "slackVars"
   ],
   "id": "c6f60b9c4edaf7c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Find two support vectors with different values of Y $(\\pm 1)$.",
   "id": "38f5194cde458858"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Sone        = None\n",
    "Smone       = None\n",
    "\n",
    "Sone_close  = 1e13\n",
    "Smone_close = 1e13\n",
    "\n",
    "for i, s in enumerate(X_train):\n",
    "    continue\n",
    "\n",
    "Sone = X_train[support_idx[0][np.where(Y_train[support_idx] > 0)[0][0]]]\n",
    "Smone = X_train[support_idx[0][np.where(Y_train[support_idx] < 0)[0][0]]]\n",
    "\n",
    "Sone, Smone"
   ],
   "id": "157a15751b755f1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Evaluate the model predictions against the actual class labels. Assess the accuracy using the metric provided by [scikit-learn's accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)",
   "id": "cf2d52f04c990ae5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for b_i in range(len(b)):\n",
    "  y_pred  = []\n",
    "  for x in X_test:\n",
    "      if np.dot(W, x) + b[b_i] < 0:\n",
    "          y_pred.append(-1)\n",
    "      else:\n",
    "          y_pred.append(1)\n",
    "y_pred  = []\n",
    "for x in X_test:\n",
    "  if np.dot(W, x) + b[0] < 0:\n",
    "    y_pred.append(-1)\n",
    "  else:\n",
    "    y_pred.append(1)"
   ],
   "id": "3b4fb64cdbef8efa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Plot all",
   "id": "3403aa1b36cca582"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax     = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# define the calculated points of decision line crossing\n",
    "c           = -b[0]        / W[1]\n",
    "c1          = -(b[0] - 1)  / W[1]\n",
    "c2          = -(b[0] + 1)  / W[1]\n",
    "a           = -W[0]     / W[1]\n",
    "print(\"Crossing point c = \", c, \"Tangent coefficient a =\", a)\n",
    "\n",
    "# perpendicular function to W - this can be defined for any support vectors as well and is\n",
    "# a decision line\n",
    "fp  =   lambda x: a * x + c\n",
    "fp1 =   lambda x: a * x + c1\n",
    "fp2 =   lambda x: a * x + c2\n",
    "\n",
    "# a lambda for parallel function to W (contains the vector W)\n",
    "f   =   lambda x: -1.0/a * x + c\n",
    "\n",
    "# set xrange\n",
    "x   =   np.arange(-2, 2, 0.01)\n",
    "\n",
    "# plot decision function\n",
    "ax.plot(    x,\n",
    "            fp(x),\n",
    "            color       =   'green',\n",
    "            alpha       =   0.3,\n",
    "            linewidth   =   7,\n",
    "            label       =   f'$f(x)$ decision line for $b=${b[0]:.4f}')\n",
    "\n",
    "# first category (+1)\n",
    "ax.plot(    x,\n",
    "            fp1(x),\n",
    "            linestyle   =   ':',\n",
    "            color       =   'blue',\n",
    "            alpha       =   0.3,\n",
    "            linewidth   =   3,\n",
    "            label       =   f'$f(x)$ lower width of the street')\n",
    "# fill the values below lines\n",
    "ax.fill_between(x, x.min(), fp1(x), color = 'blue', alpha = 0.3)\n",
    "\n",
    "# second category (-1)\n",
    "ax.plot(    x,\n",
    "            fp2(x),\n",
    "            linestyle   =   ':',\n",
    "            color       =   'red',\n",
    "            alpha       =   0.3,\n",
    "            linewidth   =   3,\n",
    "            label       =   f'$f(x)$ upper width of the street')\n",
    "ax.fill_between(x, fp2(x), x.max(), color = 'red', alpha = 0.3)\n",
    "\n",
    "# plot the weights function\n",
    "ax.plot(    x,\n",
    "            f(x),\n",
    "            color       =   'black',\n",
    "            alpha       =   0.3,\n",
    "            linewidth   =   7,\n",
    "            label       =   r'$\\vec{W}$ for $b=$'+ f'{b[0]:.4f}')\n",
    "\n",
    "# plot W vector\n",
    "norm    =   np.sqrt(np.sum(np.square(W)))\n",
    "ax.arrow(   0,\n",
    "            c,\n",
    "            W[0]   /   norm,\n",
    "            W[1]   /   norm,\n",
    "            head_width  =   0.1,\n",
    "            width       =   0.02,\n",
    "            head_length =   0.1,\n",
    "            color       =   'black')\n",
    "\n",
    "# plot labels\n",
    "ax.text(W[0]    /   norm + 0.2,\n",
    "        W[1]    /   norm - 0.1,\n",
    "        r'$\\vec{W}$',\n",
    "        fontsize=20)\n",
    "\n",
    "# to plot width\n",
    "#ax.text(0 - 1.8, c ,\n",
    "#        f'width={np.dot(Sone - Smone, W) / norm : .1e}',\n",
    "#        rotation = np.math.tanh(a) * 360 / 2 / np.pi)\n",
    "\n",
    "\n",
    "\n",
    "# scatter the points, mark the outliers with x\n",
    "ax.scatter(np.array(X1)[:,0], np.array(X1)[:,1],        marker = \"o\",    color = 'red')\n",
    "ax.scatter(np.array(X1_o)[:,0], np.array(X1_o)[:,1],    marker = \"x\",    color = 'red')\n",
    "ax.scatter(np.array(X2)[:,0], np.array(X2)[:,1],        marker = \"o\",    color = 'blue')\n",
    "ax.scatter(np.array(X2_o)[:,0], np.array(X2_o)[:,1],    marker = \"x\",    color = 'blue')\n",
    "\n",
    "# plot the support vectors\n",
    "for alpha, YY, XX in supports:\n",
    "        c = 'red' if (YY == -1) else 'blue'\n",
    "        ax.scatter(XX[0], XX[1], marker='s', color = c)\n",
    "ax.set_title(f\"Accuracy={accuracy_score(y_pred, Y_test):.2e}\")\n",
    "\n",
    "# mark the tests\n",
    "colors = ['blue' if (x == 1) else 'red' for x in y_pred]\n",
    "ax.scatter(np.array(X_test)[:,0], np.array(X_test)[:,1],\n",
    "                marker = \"x\", color = colors, label = \"Test variables\")\n",
    "\n",
    "\n",
    "ax.axhline(0, color = 'grey')\n",
    "ax.axvline(0, color = 'grey')\n",
    "ax.set_ylabel('$X_1$')\n",
    "ax.set_xlabel('$X_0$')\n",
    "ax.set_xlim(x.min(), x.max())\n",
    "ax.set_ylim(-2, 2)\n",
    "plt.legend(frameon = True)"
   ],
   "id": "90646c0c894bb7e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### e) For a given data use the SVM from [scikit-learn](https://scikit-learn.org/stable/modules/svm.html). Use different kernels and compare the results.\n",
    "- plot the support vectors\n",
    "- test the accuracy for different kernels\n",
    "- check other parameters in the documentation"
   ],
   "id": "a0dade4c6e92580f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.xlim(-2,2)\n",
    "plt.ylim(-4,4)\n",
    "\n",
    "# elipse\n",
    "decision_function = lambda x, a, b: b*np.sqrt(1 - x**2/a**2)\n",
    "a       = 0.707\n",
    "b       = 3.14\n",
    "n       = 500\n",
    "\n",
    "X       = -2.0 + 2 * 2 * np.random.random(n)\n",
    "Y       = -4.0 + 2 * 4 * np.random.random(n)\n",
    "X_m1    = []\n",
    "X_1     = []\n",
    "Y_train = []\n",
    "X_train = []\n",
    "\n",
    "# add classes\n",
    "for i, x in enumerate(X):\n",
    "    if x**2 / a**2 + Y[i]**2 / b**2 > 1:\n",
    "        X_1.append(np.array([x, Y[i]]))\n",
    "        Y_train.append(1)\n",
    "    else:\n",
    "        X_m1.append(np.array([x, Y[i]]))\n",
    "        Y_train.append(0)\n",
    "    X_train.append(np.array([x, Y[i]]))\n",
    "\n",
    "n = 20\n",
    "X = -2.0 + 2 * 2 * np.random.random(n)\n",
    "Y = -4.0 + 2 * 4 * np.random.random(n)\n",
    "# add outliers\n",
    "for i, x in enumerate(X):\n",
    "    if x**2 / a**2 + Y[i]**2 / b**2 > 1:\n",
    "        X_m1.append(np.array([x, Y[i]]))\n",
    "        Y_train.append(0)\n",
    "    else:\n",
    "        X_1.append(np.array([x, Y[i]]))\n",
    "        Y_train.append(1)\n",
    "    X_train.append(np.array([x, Y[i]]))\n",
    "X_1     = np.array(X_1)\n",
    "X_m1    = np.array(X_m1)\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "\n",
    "X = np.arange(-2.5, 2.5, 1e-4)\n",
    "plt.plot(X, decision_function(X, a, b), linestyle = '--', color = 'black')\n",
    "plt.plot(X, -decision_function(X, a, b), linestyle = '--', color = 'black')\n",
    "\n",
    "plt.scatter(X_m1[:,0], X_m1[:,1], color = 'red', label = '0')\n",
    "plt.scatter(X_1[:,0], X_1[:,1], color = 'blue', label = '1')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ],
   "id": "de07b2e79ebb9edc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# define different kernels\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "randint = np.random.randint(0, len(kernels)-1)\n",
    "# choose random or choose by hand\n",
    "kernel  = kernels[randint]\n",
    "print(\"Kernel = \", kernel)\n",
    "\n",
    "clf     = svm.SVC(kernel=kernel)\n",
    "clf.fit(X_train, Y_train)"
   ],
   "id": "e4c29eacbb0f34f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Plot the predictions and see how it goes :).",
   "id": "72e74c4a2222b76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n       = 20\n",
    "X_test  = np.array([np.array([-2.0 + 2 * 2 * np.random.random(), -4.0 + 2 * 4 * np.random.random()]) for i in range(n)])\n",
    "Y_pred  = clf.predict(X_test)\n",
    "colors  = ['red' if i == 0 else 'blue' for i in Y_pred]\n",
    "colors"
   ],
   "id": "b843f8d5043ae22a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.xlim(-2,2)\n",
    "plt.ylim(-4,4)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'Prediction for {kernel} kernel')\n",
    "\n",
    "# plot the elipse\n",
    "X = np.arange(-2.5, 2.5, 1e-4)\n",
    "plt.plot(X, decision_function(X, a, b), linestyle = '--', color = 'black')\n",
    "plt.plot(X, -decision_function(X, a, b), linestyle = '--', color = 'black')\n",
    "\n",
    "# scatter points\n",
    "zero, one = None, None\n",
    "for i in range(n):\n",
    "    if colors[i] == 'red':\n",
    "        zero = plt.scatter(X_test[i][0], X_test[i][1], color = colors[i], label = '0')\n",
    "    else:\n",
    "        one = plt.scatter(X_test[i][0], X_test[i][1], color = colors[i], label = '1')\n",
    "\n",
    "plt.legend(handles = [zero, one])"
   ],
   "id": "97739c21cd18742"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### [**Additional homework] If you want, you can implement SVM class. It can be based on the scikit-learn class, yet must be done by yourself as a whole and sent by the end of `last tutorials`. You shall show how it works for different types of data and implement kernel trick, soft margin and hard margin. It shall also compare clustering algorithms shown during the lecture to `your SVM`.\n",
    "\n",
    "# I have done it and it is in folder `My SVM implementation`"
   ],
   "id": "d15e967cc1784ddd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d630aac45c8c09eb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
