{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First part of laboratories were conducted by <a href=\"https://github.com/makskliczkowski\">Maksymilian Kliczkowski</a>. He created notebooks we used in laboratories and he help me and my friends with understanding basic and advanced topics in machine learning. I am really glad that I had him as a tutor.\n",
    "\n",
    "In first course of ML/AI we were build ML models from the ground using math.\n",
    "\n",
    "In this notebook I will present only some task we had as a homework from notebook about scikit-learn"
   ],
   "id": "76754035731338cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Explore the data",
   "id": "4382370128955e53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 1\n",
    "a) What are the keys of the dataset? What is the type of the data in each key?"
   ],
   "id": "ff2814df9397183a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for i in range(len(iris.keys())):\n",
    "  print(list(iris.keys())[i], type(iris[list(iris.keys())[i]]))"
   ],
   "id": "a8b69f3fa2aaea7a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "b) Print the description of the dataset. Use DESCR property.",
   "id": "c573de1316356d42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "iris.DESCR",
   "id": "2a89afb01c67ef1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "c) Print the feauture and target names",
   "id": "85da958015588a9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(iris.feature_names, iris.target_names)",
   "id": "137437bb025752a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 2\n",
    "Visualize the data set using ```seaborn```. What type of plot would you use? Why?"
   ],
   "id": "e8f69965fa60d44c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = sns.load_dataset('iris')\n",
    "print(df.head())"
   ],
   "id": "c6eeba37586cdd7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.FacetGrid(df, hue =\"species\",\n",
    "              height = 6).map(plt.scatter,\n",
    "                              'sepal_length',\n",
    "                              'petal_length').add_legend()\n",
    "plt.show()"
   ],
   "id": "c0b41c5131f6eb86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Construct the training and test sets\n",
    "\n",
    "#### Task 3\n",
    "\n",
    "Load the iris data set. Split it into training and test sets. Use 30% of the data for testing. Use ```your index number``` for reproducibility. Finally print the shape of resulting data sets. Use train_test_split function to do this."
   ],
   "id": "83088bebe7c4bea1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "f017910cdca3c889"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "iris    = load_iris()\n",
    "X       = iris.data\n",
    "y       = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=253880)"
   ],
   "id": "35155c5591428cb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Exploring the training and testing datasets\n",
    "print(\"X_train's shape is   :\", X_train.shape)\n",
    "print(\"X_test's shape is    :\", X_test.shape)\n",
    "print(\"y_train's shape is   :\", y_train.shape)\n",
    "print(\"y_test's shape is    :\", y_test.shape)"
   ],
   "id": "5c8c065f0d1bad72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Train a simple classifier\n",
    "\n",
    "Now, let's build a simple ML model to verify how it works without any data processing"
   ],
   "id": "8d3573489f26106a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ],
   "id": "b34d36a022676d6e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's use the <a src=https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html>logistic regression</a> model at first.",
   "id": "8f86069298e074cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model   = LogisticRegression()\n",
    "y_train = y_train.ravel()\n",
    "model.fit(X_train, y_train)"
   ],
   "id": "ab7214e9ef6b11b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create accuracy variable from <a src = https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics> sklearn metrics</a> and use accuracy_score.",
   "id": "e90c2cc56925129"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prediction  = model.predict(X_test)\n",
    "accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "print(f'Accuaracy: {accuracy}')"
   ],
   "id": "3dcee4636ff76a97"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "OK, it seems that the model works, but we can do better. Let's try to preprocess the data.",
   "id": "ca97367b1bac3ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data preprocessing\n",
    "\n",
    "The preprocessing module from ```scikit-learn``` provides a lot of useful functions to preprocess data. The brief description of the most important functions can be found in [official documentation](https://scikit-learn.org/stable/modules/preprocessing.html)."
   ],
   "id": "a5c14444823d244f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn import preprocessing",
   "id": "4c34aac50e283230"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 4\n",
    "a ) Define the transformers for the following tasks:\n",
    "* Normalization                     - scales each feature to have unit norm\n",
    "* Standardization                   - scales each feature to have zero mean and unit variance\n",
    "* Non-linear transformation         - applies a non-linear transformation to each feature in order to achieve a Gaussian-like distribution\n",
    "* Higher order features generation  - It is used to generate higher order features from the original ones. For example, if we have two features $x_1$ and $x_2$, then the second order features will be $x_1^2$, $x_2^2$, $x_1x_2$.\n",
    "\n",
    "Use dictionaries to store the transformers."
   ],
   "id": "c56b3cdfd853bcab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Normalization (```Normalizer```)",
   "id": "c696e3cf9c83e7d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "normalizer = {}\n",
    "normalizer['l1']  = preprocessing.Normalizer(norm='l1')\n",
    "normalizer['l2']  = preprocessing.Normalizer(norm='l2')\n",
    "normalizer['max']  = preprocessing.Normalizer(norm='max')"
   ],
   "id": "9ad1ca0cf87f350c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Standardization (```StandardScaler```, ```MinMaxScaler```, ```MaxAbsScaler```, ```RobustScaler```)",
   "id": "85126c47f266dfad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "scalers     = {}\n",
    "scalers['std_scaler']       = preprocessing.StandardScaler()\n",
    "scalers['min_max_scaler']   = preprocessing.MinMaxScaler()\n",
    "scalers['max_abs_scaler']   = preprocessing.MaxAbsScaler()\n",
    "scalers['robust_scaler']    = preprocessing.RobustScaler()"
   ],
   "id": "88c59aa929176b27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Non-linear transformations (```QuantileTransformer``` - with uniform and normal distribution, ```PowerTransformer``` - with Yeo-Johnson and Box-Cox transformations)",
   "id": "5f2398ae41b214e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gaussian_transformers = {}\n",
    "gaussian_transformers['quantile_transformer']       = preprocessing.QuantileTransformer(n_quantiles=105)\n",
    "gaussian_transformers['quantile_norm_transformer']  = preprocessing.QuantileTransformer(output_distribution='normal', n_quantiles=105)\n",
    "gaussian_transformers['power_bc_transformer']       = preprocessing.PowerTransformer(method=\"box-cox\")\n",
    "gaussian_transformers['power_yj_transformer']       = preprocessing.PowerTransformer(method=\"yeo-johnson\")"
   ],
   "id": "fdbea8b82817b9a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Higher order features (```PolynomialFeatures```, ```SplineTransformer```)",
   "id": "9e663150ba123546"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "hof_transformers = {}\n",
    "hof_transformers['poly']    = preprocessing.PolynomialFeatures()\n",
    "hof_transformers['spline']  = preprocessing.SplineTransformer()"
   ],
   "id": "d208bfa9e55d86fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "b) Define custom transformer which will calculate the logarithm of the features. Use ```FunctionTransformer``` from ```sklearn.preprocessing```. You can use ```np.log``` function.",
   "id": "6f9c537e8009a4d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "custom_transformer          = preprocessing.FunctionTransformer(np.log)"
   ],
   "id": "e39159370d300b51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 5\n",
    "Apply different previously defined transformers to the data set. Which one gives the best results? Try to use different parameters and different combinations of transformers.\n",
    "\n",
    "Hint: Use the previously defined model to compare the results. Create a loop over different methods"
   ],
   "id": "cad40e8b7bffe036"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "iris    = load_iris()\n",
    "X       = iris.data\n",
    "y       = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=253880)\n",
    "y_train = y_train.ravel()\n",
    "best_accuracy   = 0\n",
    "best_configs    = []\n",
    "for g in gaussian_transformers:\n",
    "  for n in normalizer:\n",
    "    for s in scalers:\n",
    "      for h in hof_transformers:\n",
    "        data = hof_transformers[h].fit_transform(scalers[s].fit_transform(normalizer[n].fit_transform(gaussian_transformers[g].fit_transform(X_train))))\n",
    "        test = hof_transformers[h].fit_transform(scalers[s].fit_transform(normalizer[n].fit_transform(gaussian_transformers[g].fit_transform(X_test))))\n",
    "        model.fit(data, y_train)\n",
    "        prediction  = model.predict(test)\n",
    "        accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "        best_configs.append([n,s,h,g, accuracy])\n"
   ],
   "id": "b523493e6ac9242"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for normalizer_name, scaler_name, hof_transformer_name, gauss, accuracy in best_configs:\n",
    "    print(f'Normalizer: {normalizer_name}, Scaler: {scaler_name}, HOF Transformer: {hof_transformer_name}, Gaussian: {gauss}, Accuracy: {accuracy}')"
   ],
   "id": "341bb38507547fb0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Different model impact",
   "id": "ec608095ca75bcd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model               = DecisionTreeClassifier(max_depth=3, random_state=100)\n",
    "model.fit(X_train, y_train)\n",
    "prediction          = model.predict(X_test)\n",
    "accuracy            = metrics.accuracy_score(y_true=y_test, y_pred=prediction)\n",
    "print(f'Non normalized data: {accuracy}')\n",
    "normalizer  = preprocessing.Normalizer()\n",
    "model               = DecisionTreeClassifier(max_depth=3, random_state=100)\n",
    "X_train_transformed = normalizer.fit_transform(X_train)\n",
    "X_test_transformed  = normalizer.transform(X_test)\n",
    "model.fit(X_train_transformed, y_train)\n",
    "prediction          = model.predict(X_test_transformed)\n",
    "accuracy            = metrics.accuracy_score(y_true=y_test, y_pred=prediction)\n",
    "print(f'Normalized data: {accuracy}')"
   ],
   "id": "1d49201fbe8dfef9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 6\n",
    "Fill the missing values for the following numpy array using ```SimpleImputer```."
   ],
   "id": "46771962edef6ceb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X = np.random.uniform(0, 10, size = (10, 2))\n",
    "X[np.random.randint(0, 10, size = 5), np.random.randint(0, 2, size = 5)] = np.nan"
   ],
   "id": "328f441b219e194b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean').fit(X)\n",
    "\n",
    "print(imp.transform(X))"
   ],
   "id": "32688cba00c88a67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## <a src= https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html> Pipelines </a> <- go on, read more if you want",
   "id": "69ba735d0c274d9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/MicrosoftDocs/ml-basics/master/data/daily-bike-share.csv')\n",
    "data.dtypes"
   ],
   "id": "6c172b98fb261cb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data.head()",
   "id": "b7fda636f4d29a7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = data[['season'\n",
    "            , 'yr'\n",
    "             , 'mnth'\n",
    "             , 'holiday'\n",
    "             , 'weekday'\n",
    "             , 'workingday'\n",
    "             , 'weathersit'\n",
    "             , 'temp'\n",
    "             , 'atemp'\n",
    "             , 'hum'\n",
    "             , 'windspeed'\n",
    "             , 'rentals']]\n",
    "data"
   ],
   "id": "ea2b4859cc9be2d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 7\n",
    "Construct a training and test set, using 'rentals' as labels. Use 30% of the data for testing. Use ```your index``` for reproducibility. Finally print the shape of resulting data sets."
   ],
   "id": "5b60b59740acca25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X = data[['season'\n",
    "            , 'yr'\n",
    "             , 'mnth'\n",
    "             , 'holiday'\n",
    "             , 'weekday'\n",
    "             , 'workingday'\n",
    "             , 'weathersit'\n",
    "             , 'temp'\n",
    "             , 'atemp'\n",
    "             , 'hum'\n",
    "             , 'windspeed']]\n",
    "y = data['rentals']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=253880)"
   ],
   "id": "fb7840f5cdeaf5ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ],
   "id": "c3b2f403f3934330"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 8\n",
    "Construct a pipeline (```Pipeline``` from ```sklearn.pipeline```) which will perform the following steps:\n",
    "* Impute missing values\n",
    "* Scale the data\n",
    "* Convert categorical features to one-hot encoding\n",
    "\n",
    "Hint:\n",
    "1) ['temp', 'atemp', 'hum', 'windspeed'] are numerical features, ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit'] are categorical features.\n",
    "\n",
    "2) Use ```ColumnTransformer``` from ```sklearn.compose``` to apply different transformers to different columns.\n",
    "\n",
    "3) Use ```OneHotEncoder``` from ```sklearn.preprocessing``` to convert categorical features to one-hot encoding. Why do we do this?\n",
    "\n",
    "4) As a model use ```LinearRegression``` from ```sklearn.linear_model```"
   ],
   "id": "8b1d30fccf8ffd4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ],
   "id": "bfd02a8f9a7cb1c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')"
   ],
   "id": "41b208ef0558426b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "numeric_features     =  ['temp', 'atemp', 'hum', 'windspeed']\n",
    "\n",
    "categorical_features =  ['season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "\n",
    "preprocessor         = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ],
   "id": "4e9dd0a01f210da2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])"
   ],
   "id": "1c431b3cb656b86c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rf_model = pipeline.fit(X_train, y_train)\n",
    "print (rf_model)"
   ],
   "id": "e35103cfdc2172fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import r2_score\n",
    "predictions = rf_model.predict(X_test)\n",
    "print(r2_score(y_test, predictions))"
   ],
   "id": "306c357b588f6748"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## WARNING! TRAGIC IMPLEMENTATION \n",
    "### Task 9\n",
    "\n",
    "\n",
    "Try different combinations of data processing methods. The highest accuracy wins. The winner gets additional 3 points. Second person gets 2 points. Third result is awarded with 1 point."
   ],
   "id": "d6d09871dd8f8556"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "iris    = load_iris()\n",
    "X       = iris.data\n",
    "y       = iris.target\n",
    "testSize = np.linspace(0.05, 0.5,46)\n",
    "best_configs    = []\n",
    "for i in testSize:\n",
    "  print(i)\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=i, random_state=253880)\n",
    "  y_train = y_train.ravel()\n",
    "\n",
    "\n",
    "  for g in gaussian_transformers:\n",
    "    for n in normalizer:\n",
    "      for s in scalers:\n",
    "        for h in hof_transformers:\n",
    "          data = hof_transformers[h].fit_transform(scalers[s].fit_transform(normalizer[n].fit_transform(gaussian_transformers[g].fit_transform(X_train))))\n",
    "          test = hof_transformers[h].fit_transform(scalers[s].fit_transform(normalizer[n].fit_transform(gaussian_transformers[g].fit_transform(X_test))))\n",
    "          model.fit(data, y_train)\n",
    "          prediction  = model.predict(test)\n",
    "          accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "          best_configs.append([n,s,h,g, accuracy, i])\n",
    "  for g in gaussian_transformers:\n",
    "          data = gaussian_transformers[g].fit_transform(X_train)\n",
    "          test = gaussian_transformers[g].fit_transform(X_test)\n",
    "          model.fit(data, y_train)\n",
    "          prediction  = model.predict(test)\n",
    "          accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "          best_configs.append([None, None, None, g, accuracy, i])\n",
    "  for n in normalizer:\n",
    "          data = normalizer[n].fit_transform(X_train)\n",
    "          test = normalizer[n].fit_transform(X_test)\n",
    "          model.fit(data, y_train)\n",
    "          prediction  = model.predict(test)\n",
    "          accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "          best_configs.append([n, None, None, None, accuracy, i])\n",
    "  for s in scalers:\n",
    "          data = scalers[s].fit_transform(X_train)\n",
    "          test = scalers[s].fit_transform(X_test)\n",
    "          model.fit(data, y_train)\n",
    "          prediction  = model.predict(test)\n",
    "          accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "          best_configs.append([None, s, None, None, accuracy, i])\n",
    "  for h in hof_transformers:\n",
    "          data = hof_transformers[h].fit_transform(X_train)\n",
    "          test = hof_transformers[h].fit_transform(X_test)\n",
    "          model.fit(data, y_train)\n",
    "          prediction  = model.predict(test)\n",
    "          accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "          best_configs.append([None, None, h, None, accuracy, i])\n",
    "  for g in gaussian_transformers:\n",
    "    for n in normalizer:\n",
    "      data = normalizer[n].fit_transform(gaussian_transformers[g].fit_transform(X_train))\n",
    "      test = normalizer[n].fit_transform(gaussian_transformers[g].fit_transform(X_test))\n",
    "      model.fit(data, y_train)\n",
    "      prediction  = model.predict(test)\n",
    "      accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "      best_configs.append([n,None,None,g, accuracy, i])\n",
    "  for g in gaussian_transformers:\n",
    "    for s in scalers:\n",
    "      data = scalers[s].fit_transform(gaussian_transformers[g].fit_transform(X_train))\n",
    "      test = scalers[s].fit_transform(gaussian_transformers[g].fit_transform(X_test))\n",
    "      model.fit(data, y_train)\n",
    "      prediction  = model.predict(test)\n",
    "      accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "      best_configs.append([None,s,None,g, accuracy, i])\n",
    "  for g in gaussian_transformers:\n",
    "    for h in hof_transformers:\n",
    "      data = hof_transformers[h].fit_transform(gaussian_transformers[g].fit_transform(X_train))\n",
    "      test = hof_transformers[h].fit_transform(gaussian_transformers[g].fit_transform(X_test))\n",
    "      model.fit(data, y_train)\n",
    "      prediction  = model.predict(test)\n",
    "      accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "      best_configs.append([n,s,h,g, accuracy, i])\n",
    "  for n in normalizer:\n",
    "    for s in scalers:\n",
    "          data = scalers[s].fit_transform(normalizer[n].fit_transform(X_train))\n",
    "          test = scalers[s].fit_transform(normalizer[n].fit_transform(X_test))\n",
    "          model.fit(data, y_train)\n",
    "          prediction  = model.predict(test)\n",
    "          accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "          best_configs.append([n,\"first \"+ s,None, None, accuracy, i])\n",
    "  for n in normalizer:\n",
    "    for s in scalers:\n",
    "          data = normalizer[n].fit_transform(scalers[s].fit_transform(X_train))\n",
    "          test = normalizer[n].fit_transform(scalers[s].fit_transform(X_test))\n",
    "          model.fit(data, y_train)\n",
    "          prediction  = model.predict(test)\n",
    "          accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "          best_configs.append([\"first \"+n,s,None, None, accuracy, i])\n",
    "  for n in normalizer:\n",
    "    for h in hof_transformers:\n",
    "      data = hof_transformers[h].fit_transform(normalizer[n].fit_transform(X_train))\n",
    "      test = hof_transformers[h].fit_transform(normalizer[n].fit_transform(X_test))\n",
    "      model.fit(data, y_train)\n",
    "      prediction  = model.predict(test)\n",
    "      accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "      best_configs.append([n, None,\"first \" + h, None, accuracy, i])\n",
    "  for n in normalizer:\n",
    "    for h in hof_transformers:\n",
    "      data = normalizer[n].fit_transform(hof_transformers[h].fit_transform(X_train))\n",
    "      test = normalizer[n].fit_transform(hof_transformers[h].fit_transform(X_test))\n",
    "      model.fit(data, y_train)\n",
    "      prediction  = model.predict(test)\n",
    "      accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "      best_configs.append([\"first \" + n, None, h, None, accuracy, i])\n",
    "  for s in scalers:\n",
    "    for h in hof_transformers:\n",
    "      data = hof_transformers[h].fit_transform(scalers[s].fit_transform(X_train))\n",
    "      test = hof_transformers[h].fit_transform(scalers[s].fit_transform(X_test))\n",
    "      model.fit(data, y_train)\n",
    "      prediction  = model.predict(test)\n",
    "      accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "      best_configs.append([None,s,\"first \" +h,None, accuracy, i])\n",
    "  for s in scalers:\n",
    "    for h in hof_transformers:\n",
    "      data = scalers[s].fit_transform(hof_transformers[h].fit_transform(X_train))\n",
    "      test = scalers[s].fit_transform(hof_transformers[h].fit_transform(X_test))\n",
    "      model.fit(data, y_train)\n",
    "      prediction  = model.predict(test)\n",
    "      accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "      best_configs.append([None,\"first \" +s,h,None, accuracy, i])\n",
    "  for n in normalizer:\n",
    "      for s in scalers:\n",
    "        for h in hof_transformers:\n",
    "          data = hof_transformers[h].fit_transform(scalers[s].fit_transform(normalizer[n].fit_transform(X_train)))\n",
    "          test = hof_transformers[h].fit_transform(scalers[s].fit_transform(normalizer[n].fit_transform(X_test)))\n",
    "          model.fit(data, y_train)\n",
    "          prediction  = model.predict(test)\n",
    "          accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "          best_configs.append([n,\"second \"+s,\"first \"+h, None, accuracy, i])\n",
    "  for n in normalizer:\n",
    "      for s in scalers:\n",
    "        for h in hof_transformers:\n",
    "          data = hof_transformers[h].fit_transform(normalizer[n].fit_transform(scalers[s].fit_transform(X_train)))\n",
    "          test = hof_transformers[h].fit_transform(normalizer[n].fit_transform(scalers[s].fit_transform(X_test)))\n",
    "          model.fit(data, y_train)\n",
    "          prediction  = model.predict(test)\n",
    "          accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "          best_configs.append([\"second \"+n,s,\"first \"+h, None, accuracy, i])\n",
    "  for n in normalizer:\n",
    "      for s in scalers:\n",
    "        for h in hof_transformers:\n",
    "          data = scalers[s].fit_transform(normalizer[n].fit_transform(hof_transformers[h].fit_transform(X_train)))\n",
    "          test = scalers[s].fit_transform(normalizer[n].fit_transform(hof_transformers[h].fit_transform(X_test)))\n",
    "          model.fit(data, y_train)\n",
    "          prediction  = model.predict(test)\n",
    "          accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "          best_configs.append([\"second \"+n,\"first \"+s,h, None, accuracy, i])\n",
    "  for n in normalizer:\n",
    "      for s in scalers:\n",
    "        for h in hof_transformers:\n",
    "          data = scalers[s].fit_transform(hof_transformers[h].fit_transform(normalizer[n].fit_transform(X_train)))\n",
    "          test = scalers[s].fit_transform(hof_transformers[h].fit_transform(normalizer[n].fit_transform(X_test)))\n",
    "          model.fit(data, y_train)\n",
    "          prediction  = model.predict(test)\n",
    "          accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "          best_configs.append([n,\"first \"+s,\"second \"+h, None, accuracy, i])\n",
    "  for n in normalizer:\n",
    "      for s in scalers:\n",
    "        for h in hof_transformers:\n",
    "          data = normalizer[n].fit_transform(hof_transformers[h].fit_transform(scalers[s].fit_transform(X_train)))\n",
    "          test = normalizer[n].fit_transform(hof_transformers[h].fit_transform(scalers[s].fit_transform(X_test)))\n",
    "          model.fit(data, y_train)\n",
    "          prediction  = model.predict(test)\n",
    "          accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "          best_configs.append([\"first \"+n,s,\"second \"+h, None, accuracy, i])\n",
    "  for n in normalizer:\n",
    "      for s in scalers:\n",
    "        for h in hof_transformers:\n",
    "          data = normalizer[n].fit_transform(scalers[s].fit_transform(hof_transformers[h].fit_transform(X_train)))\n",
    "          test = normalizer[n].fit_transform(scalers[s].fit_transform(hof_transformers[h].fit_transform(X_test)))\n",
    "          model.fit(data, y_train)\n",
    "          prediction  = model.predict(test)\n",
    "          accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "          best_configs.append([\"first \"+n,\"second \"+s,h, None, accuracy, i])\n",
    "  for g in gaussian_transformers:\n",
    "    for n in normalizer:\n",
    "      for s in scalers:\n",
    "        for h in hof_transformers:\n",
    "          data = hof_transformers[h].fit_transform(scalers[s].fit_transform(normalizer[n].fit_transform(X_train)))\n",
    "          test = hof_transformers[h].fit_transform(scalers[s].fit_transform(normalizer[n].fit_transform(X_test)))\n",
    "          model.fit(data, y_train)\n",
    "          prediction  = model.predict(test)\n",
    "          accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "          best_configs.append([n,\"second \"+s,\"first \"+h, g, accuracy, i])\n",
    "    for n in normalizer:\n",
    "      for s in scalers:\n",
    "        for h in hof_transformers:\n",
    "          data = hof_transformers[h].fit_transform(normalizer[n].fit_transform(scalers[s].fit_transform(X_train)))\n",
    "          test = hof_transformers[h].fit_transform(normalizer[n].fit_transform(scalers[s].fit_transform(X_test)))\n",
    "          model.fit(data, y_train)\n",
    "          prediction  = model.predict(test)\n",
    "          accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "          best_configs.append([\"second \"+n,s,\"first \"+h, g, accuracy, i])\n",
    "    for n in normalizer:\n",
    "      for s in scalers:\n",
    "        for h in hof_transformers:\n",
    "          data = scalers[s].fit_transform(normalizer[n].fit_transform(hof_transformers[h].fit_transform(X_train)))\n",
    "          test = scalers[s].fit_transform(normalizer[n].fit_transform(hof_transformers[h].fit_transform(X_test)))\n",
    "          model.fit(data, y_train)\n",
    "          prediction  = model.predict(test)\n",
    "          accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "          best_configs.append([\"second \"+n,\"first \"+s,h, g, accuracy, i])\n",
    "    for n in normalizer:\n",
    "      for s in scalers:\n",
    "        for h in hof_transformers:\n",
    "          data = scalers[s].fit_transform(hof_transformers[h].fit_transform(normalizer[n].fit_transform(X_train)))\n",
    "          test = scalers[s].fit_transform(hof_transformers[h].fit_transform(normalizer[n].fit_transform(X_test)))\n",
    "          model.fit(data, y_train)\n",
    "          prediction  = model.predict(test)\n",
    "          accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "          best_configs.append([n,\"first \"+s,\"second \"+h, g, accuracy, ])\n",
    "    for n in normalizer:\n",
    "      for s in scalers:\n",
    "        for h in hof_transformers:\n",
    "          data = normalizer[n].fit_transform(hof_transformers[h].fit_transform(scalers[s].fit_transform(X_train)))\n",
    "          test = normalizer[n].fit_transform(hof_transformers[h].fit_transform(scalers[s].fit_transform(X_test)))\n",
    "          model.fit(data, y_train)\n",
    "          prediction  = model.predict(test)\n",
    "          accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "          best_configs.append([\"first \"+n,s,\"second \"+h, g, accuracy, i])\n",
    "    for n in normalizer:\n",
    "      for s in scalers:\n",
    "        for h in hof_transformers:\n",
    "          data = normalizer[n].fit_transform(scalers[s].fit_transform(hof_transformers[h].fit_transform(X_train)))\n",
    "          test = normalizer[n].fit_transform(scalers[s].fit_transform(hof_transformers[h].fit_transform(X_test)))\n",
    "          model.fit(data, y_train)\n",
    "          prediction  = model.predict(test)\n",
    "          accuracy    =  metrics.accuracy_score(y_test, prediction)\n",
    "          best_configs.append([\"first \"+n,\"second \"+s,h, g, accuracy, i])"
   ],
   "id": "ec99f87c3e9e3998"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pandas_dataframe = pd.DataFrame(best_configs, columns=[\"Normalizer\", \"Scaler\", \"Hof transformer\", \"Gaussian\", \"Accuracy\", \"testSize\"])\n",
    "\n",
    "pandas_dataframe.loc[pandas_dataframe['Accuracy']==pandas_dataframe['Accuracy'].max()]"
   ],
   "id": "1b5817fd0dd507b5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
