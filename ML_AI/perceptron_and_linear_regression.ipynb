{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First part of laboratories were conducted by <a href=\"https://github.com/makskliczkowski\">Maksymilian Kliczkowski</a>. He created notebooks we used in laboratories and he help me and my friends with understanding basic and advanced topics in machine learning. I am really glad that I had him as a tutor.\n",
    "\n",
    "In first course of ML/AI we were build ML models from the ground using math.\n",
    "\n",
    "In this notebook I will present only some task we had as a homework from notebook about scikit-learn"
   ],
   "id": "80c0bf627637f71f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Perceptron as Perc"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Given is a perceptron with predefined weights:\n",
    "- $ w _0 = b = 2 $ (bias),\n",
    "- $ w_1 = 1$,\n",
    "- $ w_2 = 1$\n",
    "\n",
    "a) What is the dimension of the original input vector of the perceptron?\n",
    "\n",
    "b) Write the weights as a numpy vector and check what is the output for vectors for range of $[-2, 2]$ for each of the dimensions.\n",
    "\n",
    "What separation does this perceptron represent?\n",
    "- plot the separation as a 2D plot based on the output of the Perceptron."
   ],
   "id": "8f1e9c85f9db910e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "w = np.array([2,1,1])\n",
    "x = np.array([[1,-2,2], [1,2,2]])\n",
    "y = w.dot(x.T)"
   ],
   "id": "b3c533f266fbfd49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Construct your own perceptron class.\n",
    "It should have the field for the dimension and have the setters and getters implemented. It should also implement the multiplication treated as the dot product (what the activation function acts on). For now, keep the activation function to be defined as before.\n",
    "\n",
    "- Implement all the methods.\n",
    "- Test the class."
   ],
   "id": "d0bfd288b9145dfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Perceptron:\n",
    "    # Initialize the bias and the dimension of the perceptron.\n",
    "    # Initialize the weights to be zero (for now at least)\n",
    "    def __init__(self, W : np.array,  b = 0):\n",
    "        self.W = W\n",
    "        self.N = len(W)\n",
    "        self.b = b\n",
    "\n",
    "    # Create getters for the perceptron.\n",
    "    def get_N(self):\n",
    "      return self.N\n",
    "\n",
    "    def get_b(self):\n",
    "      return self.b\n",
    "\n",
    "    def get_W(self):\n",
    "      return self.W\n",
    "\n",
    "    ############################## SETTERS ##############################\n",
    "\n",
    "    # Create setters for the perceptron\n",
    "    def set_N(self, N):\n",
    "        self.N = N\n",
    "\n",
    "    def set_b(self, b):\n",
    "        self.b = b\n",
    "\n",
    "    def set_W(self, W : np.array):\n",
    "        self.W = W\n",
    "\n",
    "    ############################## GETTERS ##############################\n",
    "\n",
    "    # override __getitem__ method to obtain the weight at a certain position\n",
    "    # remember that you need also to override __setitem__, __getslice__\n",
    "    def __getitem__(self, key=None):\n",
    "      if key == None:\n",
    "        return self.W\n",
    "      else:\n",
    "        return self.W[key]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "      self.W[key]= value\n",
    "\n",
    "    def __getslice(self, i, j):\n",
    "        return self.W[i:j]\n",
    "\n",
    "    # set the string output of the perceptron\n",
    "    def __str__(self):\n",
    "        return f\"Am a perceptron of N={self.N} dimension{'s' if self.N > 1 else ''} biased with b={self.b}\"\n",
    "\n",
    "    ######################### OPERATORS OVERRIDE ########################\n",
    "\n",
    "    # override multiplication operators to be the activation function on other\n",
    "    def __mul__(self, other):\n",
    "      return self.activation_function(other)\n",
    "    def __rmul__(self, other):\n",
    "      return self.activation_function(other)\n",
    "\n",
    "\n",
    "    ######################### PERCEPTRON METHODS #########################\n",
    "\n",
    "    # implement the net_output of signals -> multiplication (weighted sum)\n",
    "    def net_output(self, X):\n",
    "      return self.W.dot(X.T) + self.b\n",
    "\n",
    "    # implement the activation function, use np.where and reshape it correctly - (len(X), 1)!\n",
    "    def activation_function(self, X):\n",
    "        return np.matrix(np.where(self.net_output(X) < 0, 0, 1)).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.activation_function(X)\n"
   ],
   "id": "77d40089d6d23b36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test it ;)",
   "id": "ebbb2a59fcda5a3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "a = Perceptron(np.array([0,1,2]), 5)\n",
    "b = np.array([[1,2,3], [-11,-11,-1]])\n",
    "a.predict(b)"
   ],
   "id": "3772f058774909d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Set the appropriate weights for the perceptron to reproduce this table:\n",
    "\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "    \\text{Training example} & x_1 & x_2 & \\text{Classification} \\\\\n",
    "    A&0&1&0\\\\\n",
    "    B&2&0&0\\\\\n",
    "    C&1&1&1\n",
    "\\end{array}\n"
   ],
   "id": "7deacb155fd45c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# make a perceptron with correct weights\n",
    "a = Perceptron(np.array([1, 2]), -3)\n",
    "a.predict(np.array([[0,1], [2,0], [1,1]]))"
   ],
   "id": "d9d3a444e4fd45ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Write a program that uses perceptrons implemented above to realize the binary gates:\n",
    "\n",
    "- [NAND](https://en.wikipedia.org/wiki/NAND_gate)\n",
    "- NOT\n",
    "- [AND](https://en.wikipedia.org/wiki/AND_gate)\n",
    "- [OR](https://en.wikipedia.org/wiki/OR_gate)\n",
    "- [XOR](https://en.wikipedia.org/wiki/XOR_gate)\n",
    "\n",
    "Use the lambda functions that utilize Perceptrons with the weights that allow it to act as a specific gate."
   ],
   "id": "663e117f75f45fb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### NAND",
   "id": "9e8c9713e074956a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# here is the vector of double bit inputs\n",
    "X = np.array([[0., 0],\n",
    "              [1., 0],\n",
    "              [0., 1],\n",
    "              [1., 1]])\n",
    "P = Perceptron(np.array([-1,-1]), 1.5)\n",
    "P.predict(X)"
   ],
   "id": "ffd885d5f9c4ce9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### NOT - SINGLE BIT GATE\n",
    "$$ w_1 * 0 + b >= 0 $$\n",
    "$$ w_1 * 1 + b < 0 $$"
   ],
   "id": "6479c4ca556ba1b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# NOT - EXAMPLE FOR Y'ALL\n",
    "p_not = lambda x : Perceptron(np.array([-2.5]), 2) * x\n",
    "p_not(np.array([[0],[1]]))"
   ],
   "id": "d8a106e64d297a24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### AND\n",
    "$$ w_1 * 0 + w_2 * 0 + b < 0$$\n",
    "$$ w_1 * 1 + w_2 * 0 + b < 0$$\n",
    "$$ w_1 * 0 + w_2 * 1 + b < 0$$\n",
    "$$ w_1 * 1 + w_2 * 1 + b >= 0$$"
   ],
   "id": "390b73df3fc76eeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "p_and = lambda x : Perceptron(np.array([1,1]), -2) * x\n",
    "p_and(X)"
   ],
   "id": "10f10b74785dba3d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### OR\n",
    "$$ w_1 * 0 + w_2 * 0 + b < 0$$\n",
    "$$ w_1 * 1 + w_2 * 0 + b >= 0$$\n",
    "$$ w_1 * 0 + w_2 * 1 + b >= 0$$\n",
    "$$ w_1 * 1 + w_2 * 1 + b >= 0$$"
   ],
   "id": "bb73cb5a266fbaa3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "p_or = lambda x : Perceptron(np.array([1,1]), -1) * x\n",
    "p_or(X)"
   ],
   "id": "7406d251a4175d16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### XOR\n",
    "$$ w_1 * 0 + w_2 * 0 + b < 0$$\n",
    "$$ w_1 * 1 + w_2 * 0 + b >= 0$$\n",
    "$$ w_1 * 0 + w_2 * 1 + b >= 0$$\n",
    "$$ w_1 * 1 + w_2 * 1 + b < 0$$"
   ],
   "id": "95c309670fa7ab52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "p_xor = lambda x : Perceptron(np.array([-0.5, 0.5]), 3) * x\n",
    "p_xor(X)\n"
   ],
   "id": "c1884d257736f49c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can see that XOR cannot be so easily implemented. Why is that so?\n",
    "\n",
    "In order to create the XOR gate we need to combine previous outputs. Let us demonstrate that with the usage of NAND gate."
   ],
   "id": "c1be97ad644152be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# NAND - YES, YOU CAN USE TWO PERCEPTRONS!\n",
    "p_nand = lambda x: p_not(p_and(x))\n",
    "p_nand(X)"
   ],
   "id": "61f6856a27ff3ccf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![image.png](data:image/png;base64,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)",
   "id": "e3a279b92836db47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# WORKING XOR\n",
    "p_xor = lambda x: p_and(np.concatenate((p_nand(X), p_or(X)),axis=1))\n",
    "p_xor(X)"
   ],
   "id": "21b5dbffbef27504"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. `HOMEWORK` Build Logic class that has all the methods above put together.\n",
    "1. Use it to build a [Half-adder](https://www.geeksforgeeks.org/half-adder-in-digital-logic/) (hint: you may implement combination of outputs to be an imput to other array).\n",
    "2. Implement Half-adder using NAND gates only.\n",
    "3. [*] (Additional) With the usage of our Perceptron gates implement 4 BIT PISO memory shift register. Use the for loop with wait condition to imitate the time passed. Create a simple animation and block diagram (5pts)."
   ],
   "id": "44e8c76a80c28077"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from functools import WRAPPER_ASSIGNMENTS\n",
    "# all together\n",
    "\n",
    "'''\n",
    "Use this class to store all the previously defined logical gates\n",
    "'''\n",
    "class Logic:\n",
    "\n",
    "    # combine the outputs of two gates to be an input to another gate again\n",
    "    def Comb(out1, out2):\n",
    "        return np.concatenate([out1, out2], axis = 1)\n",
    "    # take the input of the left side only\n",
    "    def Left(x):\n",
    "        return np.hsplit(x, 2)[0]\n",
    "    # take the input of the right side only\n",
    "    def Right(x):\n",
    "        return np.hsplit(x, 2)[1]\n",
    "\n",
    "    ########################### BASIC LOGIC ###########################\n",
    "    def Not(x):\n",
    "      return Perceptron(np.array([-2.5]), 2) * x\n",
    "\n",
    "    def Or(x):\n",
    "      return Perceptron(np.array([1,1]), -1) * x\n",
    "\n",
    "    def And(x):\n",
    "      return Perceptron(np.array([1,1]), -2) * x\n",
    "\n",
    "    def Nand(x):\n",
    "      return(Logic.Not(Logic.And(x)))\n",
    "\n",
    "    def Xor(x):\n",
    "       return Logic.And(np.concatenate((Logic.Nand(X), Logic.Or(X)),axis=1))\n",
    "\n",
    "    # implement the half_adder\n",
    "    def H_A(x):\n",
    "      sum = Logic.Xor(x)\n",
    "      carry = Logic.And(x)\n",
    "      return sum, carry\n",
    "    # implement the half_adder using NAND only\n",
    "    def H_A_N(x):\n",
    "        nand1   = Logic.Nand(x)\n",
    "        nand2   = Logic.Nand(Logic.Comb(Logic.Left(x),nand1))\n",
    "        nand3   = Logic.Nand(Logic.Comb(Logic.Right(x),nand1))\n",
    "        sum    = Logic.Nand(Logic.Comb(nand2, nand3))\n",
    "        carry   = Logic.Nand(Logic.Comb(nand1, nand1))\n",
    "        return sum, carry\n",
    "\n",
    "    def D_Flip_Flop(D, CLK, Q, Q_n):\n",
    "      if CLK == [[1]]:\n",
    "        if D == [[0]]:\n",
    "          return D, 1\n",
    "        else:\n",
    "          return D, 0\n",
    "      else:\n",
    "        return Q, Q_n\n",
    "\n",
    "    def PISO(D : np.array, W_S: np.array, CLKs : np.array):\n",
    "      Q_1 = [[None]]\n",
    "      Q_2 = [[None]]\n",
    "      Q_3 = [[None]]\n",
    "      Q_4 = [[None]]\n",
    "      Q_n = [[None]]\n",
    "      nand3_prev, nand6_prev, nand9_prev = [[0]],[[0]], [[0]]\n",
    "      i= 0\n",
    "      output = []\n",
    "      for CLK in CLKs:\n",
    "        Q_1, Q_n = Logic.D_Flip_Flop(D[0], CLK, Q_1, Q_n)\n",
    "        nand1 = Logic.Nand(Logic.Comb(W_S[i], Q_1))\n",
    "        nand2 = Logic.Nand(Logic.Comb(D[1], Logic.Not(W_S[i])))\n",
    "        nand3 = Logic.Nand(Logic.Comb(nand1, nand2))\n",
    "        Q_2, Q_n = Logic.D_Flip_Flop(nand3_prev, CLK, Q_2, Q_n)\n",
    "        nand4 = Logic.Nand(Logic.Comb(D[2], Logic.Not(W_S[i])))\n",
    "        nand5 = Logic.Nand(Logic.Comb(W_S[i], Q_2))\n",
    "        nand6 = Logic.Nand(Logic.Comb(nand4, nand5))\n",
    "        Q_3, Q_n = Logic.D_Flip_Flop(nand6_prev, CLK, Q_3, Q_n)\n",
    "        nand7 = Logic.Nand(Logic.Comb(D[3], Logic.Not(W_S[i])))\n",
    "        nand8 = Logic.Nand(Logic.Comb(W_S[i], Q_3))\n",
    "        nand9 = Logic.Nand(Logic.Comb(nand7, nand8))\n",
    "        Q_4, Q_n = Logic.D_Flip_Flop(nand9_prev, CLK, Q_4, Q_n)\n",
    "        nand3_prev, nand6_prev, nand9_prev = nand3, nand6, nand9\n",
    "        output.append(Q_4)\n",
    "        i+=1\n",
    "      return np.array(output)"
   ],
   "id": "fdaa060b318bf834"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![image.png](data:image/png;base64,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)",
   "id": "298cc2d63176f37f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X, Logic.Left(X), Logic.Right(X), Logic.And(X), Logic.Or(X), Logic.Xor(X), Logic.H_A(X), Logic.H_A_N(X), Logic.Nand(X)",
   "id": "821baebb1086ce81"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### PISO",
   "id": "39fe64121e937e77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "D = np.matrix([1,0,0,1])\n",
    "W_S = np.matrix([1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "CLK = np.matrix([1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0])\n",
    "out = Logic.PISO(D.T, W_S.T, CLK.T)\n",
    "CLK = np.array(CLK)[0]\n",
    "W_S = np.array(W_S)[0]"
   ],
   "id": "471ea3bb056b6877"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Animation",
   "id": "43e9036ec324080"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import rc\n",
    "rc('animation', html='jshtml')\n",
    "\n",
    "\n",
    "o = []\n",
    "c = []\n",
    "w = []\n",
    "time = []\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1)\n",
    "for ax in [ax1, ax2, ax3]:\n",
    "\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "    ax.set_xlim([0, 500])\n",
    "    ax.grid()\n",
    "\n",
    "c_line, = ax1.plot([],[])\n",
    "w_line, = ax2.plot([],[])\n",
    "o_line, = ax3.plot([],[])\n",
    "line=[c_line, w_line, o_line]\n",
    "\n",
    "def animate(i):\n",
    "  for k in range(20):\n",
    "    time.append(i*20+k)\n",
    "    plt.xlim([0, 100])\n",
    "    o.append(np.array(out)[i][0][0])\n",
    "    c.append(np.array(CLK)[i])\n",
    "    w.append(np.array(W_S)[i])\n",
    "    line[0].set_data(time,c)\n",
    "    line[1].set_data(time,w)\n",
    "    line[2].set_data(time,o)\n",
    "  return line\n",
    "ani = animation.FuncAnimation(fig, animate, blit=True, interval=10, repeat=False, frames=34)\n",
    "ani"
   ],
   "id": "c87bc47748558cd3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. Perceptron training",
   "id": "24449e725988718c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### a) For arbitrary function $f(x) : \\mathbb{R}^N \\rightarrow \\mathbb{R}^M$ create a gradient descent algorithm that tracks the history of iterations.\n",
    "Function shall return the minimum of the mapping.\n",
    "- plot the corresponding points for one-dimensional function $f(x) = x^2 - x + 5$ for big and small learning rates.\n",
    "- mark the steps with number."
   ],
   "id": "7e7f20d4655bda8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def gradient_descent(start, gradient, lr = 0.01, max_iter = 1000, tol = 0.01):\n",
    "  steps = [start]\n",
    "  for _ in range(max_iter):\n",
    "      result = gradient(start)\n",
    "      if result == 0:\n",
    "        return steps, start\n",
    "      else:\n",
    "        start -= lr * result\n",
    "      steps.append(start)\n",
    "  return steps, start"
   ],
   "id": "c677c4cf4dd2ba6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fun     = lambda x : x**2 - x + 5\n",
    "diffun  = lambda x : 2*x - 1\n",
    "start   = np.random.randint(0, 5)\n",
    "history_small, result_small     = gradient_descent(start, diffun,\n",
    "                                                   lr       =   0.01,\n",
    "                                                   max_iter =   15)\n",
    "history_big, result_big         = gradient_descent(start, diffun,\n",
    "                                                   lr       =   0.8,\n",
    "                                                   max_iter =   15)\n",
    "print(history_small, result_small)\n",
    "print(\"__________________________\")\n",
    "print(history_big, result_big)"
   ],
   "id": "d4d6648ee43ca3ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Graph",
   "id": "6db6fff006ff739a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "xval    = np.linspace(min(history_big), max(history_big), 500)\n",
    "yval_s  = fun(np.array(history_small))\n",
    "yval_b  = fun(np.array(history_big))"
   ],
   "id": "46b50cffa0848220"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.plot(xval, fun(xval), color = 'black', linestyle = '-')\n",
    "for i, txt in enumerate(history_small):\n",
    "    plt.annotate(str(i), (history_small[i], yval_s[i]), color = 'red')\n",
    "\n",
    "for i, txt in enumerate(history_big):\n",
    "    plt.annotate(str(i), (history_big[i], yval_b[i]), color = 'blue')\n",
    "\n",
    "plt.plot(history_small, yval_s, color = 'red', linestyle = '--', label = '$\\eta = 0.01$')\n",
    "plt.plot(history_big, yval_b, color = 'blue', linestyle = ':', label = '$\\eta = 0.8$')\n",
    "plt.scatter(0.5, fun(0.5), s = 85, color = 'green')\n",
    "plt.legend()"
   ],
   "id": "18639a8df484e19d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### b) Linear regression\n",
    "For the situation of linear regression (activation function is $f(x)=x$), implement the previous perceptron that can be taught to adjust its weights.\n",
    "1. set the activation function to be $f(x) = x$,\n",
    "2. implement fit(X, y) method that given training dataset $\\{(\\vec{X}_i, y_i)\\}$ and starting from a random state adjusts the weights to predict the values the best as possible. Use previous gradient descent attitude.\n",
    "3. in order to do so, implement the loss function method to be $L_{MSE}$. '\n",
    "4. for linear regression, gradient of the loss function can be calculated analyticaly - reference the lecture notes.\n",
    "5. use parameter batch if batches are to be calculated in gradient descent\n",
    "6. download the [Fish](https://www.kaggle.com/datasets/aungpyaeap/fish-market/code) dataset from Kaggle and try to fit the linear regression model to find the weight of the fish according to other columns. In order to do so, divide the dataset on 70% of train and 30% of test vectors. Use [train-test-split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) or your function.\n",
    "7. try to find the best accuracy according to the $L_{MSE}$ loss function.\n",
    "8. read the documentation of `scikit` library for [Linear regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html), compare results using it for our perceptron.\n"
   ],
   "id": "ca5acbf2f3b224fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PerceptronNew(Perceptron):\n",
    "    # Initialize the bias and the dimension of the perceptron.\n",
    "    # Initialize the weights to be zero (for now at least)\n",
    "    def __init__(self, W : np.array,  b = 0, epo = 100, lr = 0.01):\n",
    "        super().__init__(W, b)\n",
    "        self.__epo  = epo\n",
    "        self.__lr   = lr\n",
    "\n",
    "    # Create getters for the perceptron.\n",
    "    def get_lr(self):\n",
    "      return self.__lr\n",
    "    def get_epo(self):\n",
    "      return self.__epo\n",
    "\n",
    "    ############################## SETTERS ##############################\n",
    "    def set_lr(self, lr):\n",
    "      self.__lr = lr\n",
    "\n",
    "    def set_epo(self, epo):\n",
    "      self.__epo = epo\n",
    "\n",
    "\n",
    "    ############################## PERCEPTRON METHODS ##############################\n",
    "\n",
    "    # implement the activation function\n",
    "    def activation_function(self, X):\n",
    "      return np.matrix(X*self.W+self.b).T\n",
    "    # use this loss function to calculate the correctness of the perceptron - MSE\n",
    "    def loss(self, y_true : np.array, y_pred : np.array):\n",
    "      return np.mean((y_true - y_pred))\n",
    "\n",
    "\n",
    "    # give fit the parameter randomstate and whenever it is not None, the weights\n",
    "    # are reset to be random normal - this ensures random starting point of gradient descent\n",
    "    # open the lecture notes or derive it yourself!\n",
    "    def fit(self, X, y, randomstate = None, batch = 1):\n",
    "\n",
    "        if type(X) != np.ndarray:\n",
    "            X = X.to_numpy()\n",
    "        if type(y) != np.ndarray:\n",
    "            y = y.to_numpy().reshape(-1,1)\n",
    "\n",
    "        # set the appropriate weights\n",
    "        if randomstate is not None:\n",
    "            self.W = np.random.normal(0.0, 0.1, self.N)\n",
    "            self.b = np.random.normal(0.0, 1.0)\n",
    "\n",
    "        history     = []\n",
    "        bucket_num  = len(X) // batch\n",
    "        print(bucket_num)\n",
    "        # slice the input in to be multiple batches! make a lambda function or slice beforehand\n",
    "        slicing     = lambda x: np.split(x, bucket_num)\n",
    "\n",
    "        # go through epochs\n",
    "        for epo in range(self.__epo):\n",
    "            # iterate batches\n",
    "            loss    = 0.0\n",
    "            # go through bins\n",
    "            for bin in range(1, bucket_num + 1):\n",
    "                X_slice = slicing(X)[bin-1]\n",
    "                y_slice = slicing(y)[bin-1]\n",
    "\n",
    "                pred    = self.activation_function(X_slice)\n",
    "                #print(pred, y_slice)\n",
    "                #delta_i =\n",
    "                #suma_w  =\n",
    "                #suma_b  =\n",
    "\n",
    "                # calculate loss\n",
    "                loss    = self.loss(y_slice, pred)\n",
    "                # update the weights\n",
    "                self.W  += loss * self.get_lr()\n",
    "                self.b  += loss * self.get_lr()\n",
    "                #print(f'\\t->epo:{epo}:bin={bin}')\n",
    "            # average loss\n",
    "            loss    /=  bucket_num\n",
    "            print(f'epo:{epo}->loss={loss}')\n",
    "            history.append(loss)\n",
    "        return history\n",
    "\n",
    "    def plot_history(self, history):\n",
    "        plt.xlabel('epo')\n",
    "        plt.ylabel('loss')\n",
    "        plt.plot(history)"
   ],
   "id": "593a0fe73c3d6633"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### TEST LINEAR REGRESSORS ON RANDOM DATA!",
   "id": "9763c9ac0713f56f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X = np.arange(0, 100, 1)\n",
    "y = 3 * X + 5 + np.random.normal(0, 1e1, size = len(X))\n",
    "\n",
    "X = X.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "X_train = X[:int(0.8 * len(X))]\n",
    "y_train = y[:int(0.8 * len(y))]\n",
    "X_test  = X[int(0.8 * len(X)):]\n",
    "y_test  = y[int(0.8 * len(y)):]"
   ],
   "id": "67679de37ceb10f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lin_reg = PerceptronNew(np.array([5]), 1)\n",
    "lin_reg.set_lr(1e-4)\n",
    "history = lin_reg.fit(X=X_train, y=y_train, randomstate=42, batch=2)\n",
    "lin_reg.plot_history(history)"
   ],
   "id": "b5ded469efcecbe7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.scatter(X_train, y_train, color=\"blue\", s=10, label=\"train_data\")\n",
    "plt.scatter(X_test, y_test, color=\"green\", s=10, label=\"test_data\");\n",
    "plt.scatter(X_test, np.array(lin_reg.activation_function(X_test).reshape(-1,1)), color=\"red\", s=10, label=\"pred_data\")\n",
    "plt.legend();"
   ],
   "id": "61ea4ba07233ff57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### TEST LINEAR REGRESSORS ON REAL LIFE DATA",
   "id": "3115e839dfb73103"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df          = pd.read_csv('data/Fish.csv')\n",
    "print(df.dtypes)\n",
    "train_cols  = ['Length1','Length2','Length3','Height','Width']\n",
    "df"
   ],
   "id": "d4758b679fe7f49b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical_transformer = LabelEncoder()\n",
    "cols                                = [df.columns[0]] + list(df.columns[2:])\n",
    "y = df[cols[0]]\n",
    "X = df[cols[1:]]\n",
    "y = categorical_transformer.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test    = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train.shape"
   ],
   "id": "21f913b2cd5896e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y_train.shape",
   "id": "d7893dddb7563cbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "min_loss    = 1e+10\n",
    "best_lr     = None\n",
    "# train multiple learning rates and save the best one\n",
    "for lr in np.linspace(1e-6, 2e-4, 20):\n",
    "    # create our new Perceptron, initialize it with zeros\n",
    "    p       = PerceptronNew(W=np.array([0.,0.,0.,0.,0.]), b= 0., lr=lr)\n",
    "    # save the history\n",
    "    history = p.fit(X=X_train, y=y_train)\n",
    "    loss    = history[-1]\n",
    "    if loss < min_loss:\n",
    "        min_loss    = loss\n",
    "        best_lr     = lr\n",
    "    # plot history\n",
    "    p.plot_history(history)"
   ],
   "id": "95365521a87c7f09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"Best learning rate = \", best_lr)",
   "id": "b522141bf6d0f982"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_pred  = p.activation_function(X_test)\n",
    "mse     = np.mean(np.square(y_test - y_pred))\n",
    "mse"
   ],
   "id": "644f0d87e6a69e0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create a LinearRegression class from scikit-learn",
   "id": "e49102390881a109"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "reg         = LinearRegression().fit(X_train, y_train)\n",
    "y_pred_sk   = reg.predict(X_test)\n",
    "reg.score(X_test, y_test)"
   ],
   "id": "a5199f8bbc67d13b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mse_sk      = np.mean(np.square(y_pred_sk - y_test))\n",
    "mse_sk"
   ],
   "id": "e9f258b222955679"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Next labs - the same topic",
   "id": "639f354ce817f463"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Probably code delivered by Maks",
   "id": "1ff0b1cdb98ff984"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PerceptronLinear(Perceptron):\n",
    "    # Initialize the bias and the dimension of the perceptron -> but not only that\n",
    "    def __init__(self, W : np.array,  b = 0, epo = 100, lr = 0.01):\n",
    "        super().__init__(W, b)\n",
    "        # how many learning iterations we give\n",
    "        self.epo = epo\n",
    "        # what is our step in the gradient\n",
    "        self.lr = lr\n",
    "\n",
    "    ############################## GETTERS ##############################\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.lr\n",
    "\n",
    "    def get_epo(self):\n",
    "        return self.epo\n",
    "\n",
    "    ############################## SETTERS ##############################\n",
    "\n",
    "    def set_lr(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def set_epo(self, epo):\n",
    "        self.epo = epo\n",
    "\n",
    "    ############################## PERCEPTRON METHODS ##############################\n",
    "    '''\n",
    "    As you can see the activation function has been changed to linear regression. What does reshape do?\n",
    "    After you figure it out, note that it is the usual way of having the output of the machine learning algorithm. Why?\n",
    "    '''\n",
    "    def activation_function(self, X):\n",
    "        return (self.net_output(X)).reshape(-1,1)\n",
    "\n",
    "    '''\n",
    "\n",
    "    Loss function for the perceptron -> here we use the Mean Square Error\n",
    "    '''\n",
    "    def loss(self, y_true : np.array, y_pred : np.array):\n",
    "        square = np.square(y_true - y_pred)\n",
    "        return np.mean(square)\n",
    "\n",
    "    '''\n",
    "    Single step of the gradient, here it is calculated analytically (linear regression)\n",
    "    '''\n",
    "    def gradient(self, x_true, y_true, prediction):\n",
    "        delta_i = (y_true.flatten() - prediction)\n",
    "        suma_w = np.multiply(delta_i[:, np.newaxis], x_true)\n",
    "        suma_b = delta_i\n",
    "        return suma_b, suma_w\n",
    "\n",
    "    '''\n",
    "    Fit function allows to obtain the (probably most?) correct weights for the perceptron via the gradient descent algorithm.\n",
    "    '''\n",
    "    def fit(self, X, y, randomstate = None, batch = 1, verbose = False):\n",
    "\n",
    "        if type(X) != np.ndarray:\n",
    "            X = np.array(X)\n",
    "        if type(y) != np.ndarray:\n",
    "            y = np.array(y).reshape(-1,1)\n",
    "\n",
    "        # give fit the parameter randomstate and whenever it is not None, the weights\n",
    "        # are reset to be random normal - this ensures random starting point of gradient descent\n",
    "        if randomstate is not None:\n",
    "            self.W = np.random.normal(0.0, 0.1, self.N)\n",
    "            self.b = np.random.normal(0.0, 1.0)\n",
    "\n",
    "        # Save the history of the losses. Why?\n",
    "        history = []\n",
    "        # If we want to calculate the gradient in buckets (look for description of the batch)\n",
    "        bucket_num = len(X) // batch\n",
    "        # slice the data onto batches without shuffling (no stochasticity)\n",
    "        slicing = lambda x, b: x[(b-1)*batch:b*batch]\n",
    "\n",
    "        # iterate epochs\n",
    "        for epo in range(self.epo):\n",
    "            # iterate batches\n",
    "            loss = 0.0\n",
    "            for bin in range(1, bucket_num + 1):\n",
    "                X_slice = slicing(X,bin)\n",
    "                y_slice = slicing(y,bin)\n",
    "                # predict the output for a given slice (what is the shape of the output?)\n",
    "                pred = self.predict(X_slice)\n",
    "\n",
    "                suma_b, suma_w = self.gradient(X_slice, y_slice, pred.flatten())\n",
    "\n",
    "                # calculate loss\n",
    "                loss += self.loss(y_slice, pred.flatten())\n",
    "\n",
    "                # update the weights\n",
    "                self.W += np.mean(suma_w, axis = 0) * self.lr\n",
    "                self.b += np.mean(suma_b, axis = 0) * self.lr\n",
    "            # calculate average loss\n",
    "            loss/=bucket_num\n",
    "            if verbose:\n",
    "                print(f'epo:{epo}->loss={loss}')\n",
    "            history.append(loss.flatten())\n",
    "        return np.array(history).flatten()\n",
    "    '''\n",
    "    Basic history plot\n",
    "    '''\n",
    "    def plot_history(self, history, ax = None):\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "        ax.set_xlabel('epo')\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.plot(history)"
   ],
   "id": "1f0324a8f82fbc9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### b) Test the linear Perceptron on linearized data with normal noise.",
   "id": "23dfd3b5d350a4eb"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "X       = np.arange(0, 100, 1)\n",
    "y       = 3 * X + 5 + np.random.normal(0, 1e1, size = len(X))\n",
    "\n",
    "X       = X.reshape(-1, 1)\n",
    "y       = y.reshape(-1, 1)\n",
    "\n",
    "X_train = X[:int(0.8 * len(X))]\n",
    "y_train = y[:int(0.8 * len(y))]\n",
    "X_test  = X[int(0.8 * len(X)):]\n",
    "y_test  = y[int(0.8 * len(y)):]"
   ],
   "id": "3efbadbf6c2c7a8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lin_reg = PerceptronLinear([5,], 1)\n",
    "lin_reg.set_lr(1e-5)\n",
    "history = lin_reg.fit(X=X_train, y=y_train, randomstate=42, batch=3)\n",
    "lin_reg.plot_history(history)"
   ],
   "id": "70e833d88f06b685"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.scatter(X_train, y_train,\n",
    "            color=\"blue\", s=10, label=\"train_data\")\n",
    "plt.scatter(X_test, y_test,\n",
    "            color=\"green\", s=10, label=\"test_data\")\n",
    "plt.scatter(X_test, lin_reg.activation_function(X_test).reshape(-1, 1), color=\"red\", s=10, label=\"pred_data\")\n",
    "plt.legend();"
   ],
   "id": "79c4cf987d2c8376"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### c) Taking the inspiration from the work above, work out the procedure to create a Perceptron that is able to learn basic binary classification.\n",
    "\n",
    "Where is the gradient in this equation?\n",
    "- calculate the gradient analyticaly and see for yourself.\n",
    "- derive methods from PerceptronLinear"
   ],
   "id": "75a3568a26089910"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PerceptronBinary(PerceptronLinear):\n",
    "    def __init__(self, W : np.array,  b = 0, epo = 100, lr = 0.01):\n",
    "        super().__init__(W, b, epo, lr)\n",
    "\n",
    "    '''\n",
    "    $\\Phi(x) = sign(x)\n",
    "    '''\n",
    "    def activation_function(self, X):\n",
    "        return np.where(self.net_output(X) >= 0.0, 1.0, -1.0).reshape(-1,1)\n",
    "\n",
    "    '''\n",
    "    Loss function for the perceptron -> here we use knowledge that the classes can be either {-1, 1}\n",
    "    '''\n",
    "    def loss(self, y_true : np.array, y_pred : np.array):\n",
    "        square = np.square(1 - y_true * y_pred)\n",
    "        return np.mean(square)\n",
    "\n",
    "    '''\n",
    "    Single step of the gradient, here it is calculatable analytically (linear regression).\n",
    "    It shall return a numpy array for bias and for weights (which will be later summed, hence the name sum), where the elements correspond to different samples in one batch.\n",
    "    Remember that it should work also for batch updates.\n",
    "    '''\n",
    "    def gradient(self, x_true, y_true, prediction):\n",
    "        val     = np.multiply(y_true, (1.0-y_true * prediction))\n",
    "        suma_w  = np.multiply(val, x_true)\n",
    "        suma_b  = val\n",
    "        return suma_b, suma_w"
   ],
   "id": "a8ee06b47092d80"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### d) Use the Perceptron above to check, whether you were right about the weights during the previous lab!\n",
   "id": "4293c02517bc19ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "a       = PerceptronBinary(np.random.random(2), np.random.random(), 100, 1e-1)\n",
    "x_train = [[0.,1.], [2.,0.], [1.,1.], [2.,3.]]\n",
    "y_train = [-1., -1., 1.,1.]\n",
    "for d in x_train:\n",
    "    plt.scatter(d[0],d[1])\n",
    "plt.legend(y_train)"
   ],
   "id": "69a0fd98f0c1764c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "history = a.fit(x_train, y_train)\n",
    "a.plot_history(history)\n",
    "a.predict(x_train), a.get_W()"
   ],
   "id": "4e2eff4b4fb3c6ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "What happens when the data is not linearly seperable?",
   "id": "3955d1e5948cbafb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def find_orthogonal_vector(x):\n",
    "    '''\n",
    "    Given a vector x, find a vector orthogonal to it randomly\n",
    "    '''\n",
    "    x_p     =   np.random.random(len(x))\n",
    "    # take out the vector that is in line with x\n",
    "    x_p     -=  x_p.dot(x) * x / np.linalg.norm(x)**2.0\n",
    "    return x_p\n",
    "\n",
    "def find_plane_from_ort(perp1, perp2):\n",
    "    '''\n",
    "    Given two points, perp1 and perp2, find vector connecting those two points.\n",
    "    '''\n",
    "    # a * x1 + b = y1\n",
    "    # a * x2 + b = y2\n",
    "    # -> a * (x1-x2) = (y1-y2) -> a = (y1-y2)/(x1-x2)\n",
    "    a = (perp1[1] - perp2[1]) / (perp1[0] - perp2[0])\n",
    "    b = perp1[1] - a * perp1[0]\n",
    "    return a, b"
   ],
   "id": "4078e90495258c58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### e) Take 50 randomly initialized points in 2D -> $r=[x=rand(), y=rand()]$ such that $x,y \\in [-1,1]$.\n",
    "For each of them assign a class such that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} =\n",
    "\\left \\{\n",
    "   \\begin{array}{lr}\n",
    "       1 & y \\geq 3 * x - 1 \\\\\n",
    "       -1 &\\text{otherwise}\n",
    "   \\end{array}\n",
    "\\right .\n",
    "\\end{equation}\n",
    "\n",
    "Randomly initialize vector $\\vec{W}$ of the perceptron.\n",
    "\n",
    "Train the perceptron to fit the data. Use 50 epochs. At each 10th epoch plot the vector perpendicular to $\\vec{W}$ Use the functions above to find the perpendicular vector and plane (line) in 2D. What can we tell about the learning algorithm? `We will use 2D input data to visualise the behavior of learning.`\n"
   ],
   "id": "bf872d91d5285747"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x = (np.random.random(50) * 2) - 1\n",
    "y = (np.random.random(50) * 2) - 1\n",
    "r = np.stack((x, y), axis=-1)\n",
    "y_t = np.where((y >= 3*x - 1), 1.0, -1.0)\n",
    "for d in r:\n",
    "    plt.scatter(d[0],d[1])\n",
    "b       = PerceptronBinary(np.random.random(2), np.random.random(), 10, 1e-1)\n",
    "history = b.fit(r, y_t)\n",
    "a.plot_history(history)\n",
    "history = b.fit(r, y_t)\n",
    "a.plot_history(history)\n",
    "history = b.fit(r, y_t)\n",
    "a.plot_history(history)\n",
    "history = b.fit(r, y_t)\n",
    "a.plot_history(history)\n",
    "history = b.fit(r, y_t)\n",
    "a.plot_history(history)\n",
    "#a.predict(x_train), a.get_W()"
   ],
   "id": "fc81a79a7af8a9eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### [DISCLAIMER] Note that the plot of the decision bound is in fact here just conceptual as $W_0=b$ is also sought with the gradient descent and is not used for the plot.\n",
   "id": "fd5d836f3085dabc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### f) Calculate the gradient $\\frac{\\partial}{\\partial W_k}$ of the newly created loss function with the softmax activation.\n",
    "$$ L_i = \\log(1+e^{ -y^i\\vec{X}^i\\cdot \\vec{W}}). $$\n",
    "Change perceptron and use it for classification of the random data again. Which activation performs better? Try using batches."
   ],
   "id": "18e1a97fb30c0dc1"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "class PerceptronSoftmax(PerceptronLinear):\n",
    "    def __init__(self, W : np.array,  b = 0, epo = 100, lr = 0.01):\n",
    "        super().__init__(W, b, epo, lr)\n",
    "\n",
    "    '''\n",
    "    $\\Phi(x) = sign(x)\n",
    "    '''\n",
    "    def activation_function(self, X):\n",
    "        return np.where(self.net_output(X) >= 0.0, 1.0, -1.0).reshape(-1,1)\n",
    "\n",
    "\n",
    "    def loss(self, y_true : np.array, y_pred : np.array):\n",
    "        exp = np.exp(-y_true * y_pred)\n",
    "        return np.log(1+exp)\n",
    "\n",
    "    '''\n",
    "    Single step of the gradient, here it is calculatable analytically (linear regression)\n",
    "    '''\n",
    "    def gradient(self, x_true, y_true, prediction):\n",
    "        val     = np.multiply(y_true, np.exp(1.0-(y_true * prediction))) #exp missing\n",
    "        suma_w  = np.multiply(val, x_true)\n",
    "        suma_b  = val\n",
    "        return suma_b, suma_w\n",
    "        pass"
   ],
   "id": "6a20e279d57a2ea2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test it!",
   "id": "3c188c53b4c758f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(2, figsize = (10,15))\n",
    "# create separating function\n",
    "sep_fun = lambda x, y: 1 if y >= 3*x - 1 else -1\n",
    "# generate random data and asign labels\n",
    "x_train = 2.0 * np.random.random((50, 2)) - 1.0\n",
    "y_train = np.array([sep_fun(x,y) for (x,y) in x_train])\n",
    "\n",
    "\n",
    "\n",
    "# set the classes for the plot\n",
    "colors  = ['red' if y > 0 else 'blue' for y in y_train]\n",
    "labels  = ['1' if y > 0 else '-1' for y in y_train]\n",
    "ax[0].plot(x, 3*x-1, linestyle=':', label = r'True $\\vec V_{\\rm sep}$: $f(x) \\equiv y = 3x-1$')\n",
    "ax[0].scatter(x_train[:,0], x_train[:,1], c = colors)\n",
    "ax[0].set_xlim(-2,1)\n",
    "ax[0].set_ylim(-1,1)\n",
    "\n",
    "def plotTrainingPlane(  W : np.ndarray,\n",
    "                        ax,\n",
    "                        color,\n",
    "                        label):\n",
    "    '''\n",
    "    Plot the training plane finding the vectors corresponding to W!\n",
    "    - W : vectors of weights at given step in two dimensions\n",
    "    '''\n",
    "    # set the x_range to plot the real line:\n",
    "    x     = np.arange(-1, 1, 1e-2)\n",
    "    # we must find two points in order to find where the random vector goes\n",
    "    # first point can be set where -W finishes (for convenience :))\n",
    "    perp1 = -W\n",
    "    perp2 = find_orthogonal_vector(W) + perp1\n",
    "    print(perp1, perp2)\n",
    "    # find the line connecting those two points\n",
    "    plane = find_plane_from_ort(perp1, perp2)\n",
    "    # scatter the plane vectors\n",
    "    ax.scatter(x, plane[0] * x + plane[1], color = color,\n",
    "               label = 'epo=' + str(i) + r':$\\vec V _{i}$', s=5)\n",
    "    # plot the vectors that are orthogonal to this plane\n",
    "    ax.plot([0.0, W[0]],\n",
    "            [0.0, W[1]],\n",
    "            color = color,\n",
    "            ls  =  '-')\n",
    "    ax.plot([0.0, -W[0]],\n",
    "            [0.0, -W[1]],\n",
    "            marker = 'x',\n",
    "            color = color,\n",
    "            ls  =  '-')\n",
    "    ax.annotate(text = f'$W(epo={i})$', xy=[W[0], W[1]], fontsize = 14)\n",
    "\n",
    "\n",
    "# set the initial vector W to be two-dimensional random\n",
    "W       = np.random.random(2)\n",
    "# create the perceptron\n",
    "p       = p=PerceptronSoftmax(W, np.random.random(), 1, 5e-2)\n",
    "\n",
    "epo     = 60\n",
    "modulo  = 20\n",
    "history = []\n",
    "colors  = iter(['red', 'orange', 'green', 'blue', 'pink'])\n",
    "for i in range(epo):\n",
    "    if i % modulo == 0:\n",
    "        plotTrainingPlane(W, color = next(colors), ax = ax[0], label = i)\n",
    "        print(\"Accuracy: \", np.mean(p.predict(x_train).flatten() == y_train))\n",
    "\n",
    "    history = history + list(p.fit(x_train, y_train))\n",
    "    W       = p.get_W()\n",
    "ax[0].scatter([0], [0], marker = 'o', s = 40, color = 'black')\n",
    "ax[0].legend(loc = 'best', frameon = False, fontsize = 14)\n",
    "ax[0].set_title(\"Note that due to the bias, we cannot find out real corresponding plane, it's the slope that matters\")\n",
    "p.plot_history(history, ax[1])\n"
   ],
   "id": "26a4fb610b9271c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Newton's method.\n",
    "For smaller datasets it is sometimes usefull to use the Newton's method to find the function minumum with faster convergence. Suppose we have some function:\n",
    "$$ f : \\mathbb{R} \\rightarrow \\mathbb{R} .$$\n",
    "\n",
    "We try to find a value where f(x) = 0 (or as close as possible). For this we set some initial point $x_0$ and calculate $f(x_0)$. If $x_0$ is near some real point close to 0 - $\\alpha$, then the tangent line at $x_0$ crosses the x-axis closer and closer to $\\alpha$.\n",
    "\n",
    "<center><img src=https://upload.wikimedia.org/wikipedia/commons/e/e0/NewtonIteration_Ani.gif></center>\n"
   ],
   "id": "e419a52fd1a8b46d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def find_a_and_b_and0(prev_x, prev_y, diff):\n",
    "    # y_0 = a_0 * x_0 + b_0 -> b_0 = y_0 - a_0*x_0\n",
    "    a       = diff(prev_x)\n",
    "    b       = prev_y - a * prev_x\n",
    "    zero    = - b / a\n",
    "    return a, b, zero\n",
    "\n",
    "x   =np.arange(-1,1,1e-2)\n",
    "f   = lambda x : np.square(x)\n",
    "df  = lambda x : 2 * x\n",
    "\n",
    "plt.plot(x, f(x))\n",
    "plt.axvline(0.0, c='black')\n",
    "plt.axhline(0.0, c='black')\n",
    "\n",
    "x_0 = 0.75\n",
    "y_0 = f(x_0)\n",
    "plt.scatter(x_0, y_0, c='red')\n",
    "\n",
    "# find first tangent line\n",
    "a_0, b_0, x_1   = find_a_and_b_and0(x_0, y_0, df)\n",
    "y_1             = f(x_1)\n",
    "plt.plot(x[len(x)//2:], a_0 * x[len(x)//2:] + b_0)\n",
    "plt.scatter(x_1, y_1, c='red')\n",
    "\n",
    "# find second tangent line\n",
    "a_1, b_1, x_2   = find_a_and_b_and0(x_1, y_1, df)\n",
    "y_2             = f(x_2)\n",
    "plt.plot(x[len(x)//2:], a_1 * x[len(x)//2:] + b_1)\n",
    "plt.scatter(x_2, y_2, c='red')\n",
    "\n",
    "# find third tangent line\n",
    "a_2, b_2, x_3   = find_a_and_b_and0(x_2, y_2, df)\n",
    "y_3             = f(x_3)\n",
    "plt.plot(x[len(x)//2:], a_2 * x[len(x)//2:] + b_2)\n",
    "plt.scatter(x_3, y_3, c='red')"
   ],
   "id": "5dd2c6aeaf8ff1c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### e) Add the method to calculate Newton-Raphson algorithm in the previous Perceptron. In order to do this, calculate the derviative:\n",
    "\n",
    "$$ H_{ij} = \\frac{\\partial ^2 }{\\partial w_i \\partial w_j} \\log(1+e^{ -y^i\\vec{X}^i\\cdot \\vec{W}}).$$"
   ],
   "id": "bec4f4579cfd3faf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PerceptronSoftmax(PerceptronLinear):\n",
    "    def __init__(self, W : np.array,  b = 0, epo = 100, lr = 0.01):\n",
    "        super().__init__(W, b, epo, lr)\n",
    "\n",
    "    '''\n",
    "    $\\Phi(x) = sign(x)\n",
    "    '''\n",
    "    def activation_function(self, X):\n",
    "        return np.where(self.net_output(X) >= 0.0, 1.0, -1.0).reshape(-1,1)\n",
    "\n",
    "    def loss(self, y_true : np.array, y_pred : np.array):\n",
    "        pass\n",
    "\n",
    "    def gradient(self, x_true, y_true, prediction):\n",
    "        '''\n",
    "        Single step of the gradient, here it is calculatable analytically (linear regression)\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def hessian(self, x_true, y_true, y_pred):\n",
    "        hess = np.zeros((len(x_true) + 1, len(x_true) + 1))\n",
    "        pass\n",
    "\n",
    "    def newton_rap(self, X, y, randomstate = None, verbose = False):\n",
    "        '''\n",
    "        https://youtu.be/8yis7GzlXNM?si=woJFmzvTuqjYashv\n",
    "        '''\n",
    "        if type(X) != np.ndarray:\n",
    "            X = np.array(X)\n",
    "        if type(y) != np.ndarray:\n",
    "            y = np.array(y).reshape(-1,1)\n",
    "\n",
    "        # give fit the parameter randomstate and whenever it is not None, the weights\n",
    "        # are reset to be random normal - this ensures random starting point of gradient descent\n",
    "        if randomstate is not None:\n",
    "            self.W = np.random.normal(0.0, 0.1, self.N)\n",
    "            self.b = np.random.normal(0.0, 1.0)\n",
    "\n",
    "        # Save the history of the losses. Why?\n",
    "        history = []\n",
    "\n",
    "        # iterate epochs\n",
    "        for epo in range(self.epo):\n",
    "            # iterate batches\n",
    "            loss = 0.0\n",
    "            for i, X_slice in enumerate(X):\n",
    "                y_slice = y[i]\n",
    "                # predict the output for a given slice (what is the shape of the output?)\n",
    "                pred    = self.predict(X_slice)\n",
    "                suma_b, suma_w = self.gradient(X_slice, y_slice, pred.flatten())\n",
    "                # calculate the gradient\n",
    "                grad    = np.array(list(suma_b.flatten()) + list(suma_w.flatten()))\n",
    "                # calculate the hessian\n",
    "                hessian = self.hessian(X_slice, y_slice, pred.flatten())\n",
    "                # calculate the update vector (use pseudoinverse to be numerically safe)\n",
    "                update  = np.linalg.pinv(hessian).dot(grad)\n",
    "                # update weights\n",
    "                self.W  += np.array(self.lr*update[1:])\n",
    "                self.b  += np.array(self.lr*update[0])\n",
    "                # calculate loss\n",
    "                loss    += self.loss(y_slice, pred.flatten())\n",
    "\n",
    "            if verbose:\n",
    "                print(f'epo:{epo}->loss={loss}')\n",
    "            history.append(loss.flatten())\n",
    "        return np.array(history).flatten()"
   ],
   "id": "91c01fba00b496b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### h) Use the following data to test the algorithm for Newton-Rhapson\n",
    "Use 5 epochs and $\\vec{W} = [1],b=1$ as a starting point."
   ],
   "id": "9dbf3bf3d6c99b01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X = np.array([-0.3, -0.1, 0.3, 0.5, 1.0, 1.3, 4.5, 3.5]).reshape(-1,1)\n",
    "Y = np.array([-1,-1,-1, -1, 1,1,1,1],dtype=np.float64).reshape(-1,1)\n",
    "plt.plot(X,Y)\n",
    "plt.scatter(X,Y,c='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('class')"
   ],
   "id": "72339b8867a2232"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "p       =   PerceptronSoftmax(np.array([1.]),1., epo = 20)\n",
    "history =   p.newton_rap(X, Y)\n",
    "p.plot_history(history)\n",
    "p.predict(X)"
   ],
   "id": "bf60b2517a0822e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### i) Do the same for vector $\\vec{W}=[4.0], b=-10$.",
   "id": "684cf28221d7060d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "p       =   PerceptronSoftmax(np.array([4.]),-10., epo = 100)\n",
    "history =   p.newton_rap(X.reshape((-1,1)), Y.reshape((-1,1)))\n",
    "p.plot_history(history)\n",
    "p.predict(X.reshape(-1,1))"
   ],
   "id": "816d067fb9425a91"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Logistic regression",
   "id": "f4e72e189ae6de65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### k) Create the logistic regression algorithm using the information from above. Implement Newton-Rhapson for it as well.",
   "id": "e388a6b75ebab70b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import scipy as sc",
   "id": "b973c78e6e33f877"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PerceptronLogistic(PerceptronBinary):\n",
    "    def __init__(self, W : np.array,  b = 0, epo = 100, lr = 0.01):\n",
    "        super().__init__(W, b, epo, lr)\n",
    "\n",
    "    def activation_function(self, X):\n",
    "        return sc.special.expit(X*self.W).shape(-1,1)\n",
    "\n",
    "    def loss(self, y_true : np.array, y_pred : np.array):\n",
    "        lg = np.log(1 - y_pred)\n",
    "        mul = np.multiply((1-y_true), lg)\n",
    "        lh = np.multiply(y_true, np.log(y_pred))\n",
    "        return lh + mul\n",
    "\n",
    "    def gradient(self, x_true, y_true, prediction):\n",
    "      sig = self.activation_function(np.multiply(self.W * x_true))\n",
    "      return np.multiply((y_true - sig), x_true)"
   ],
   "id": "bae3ae85ca26ff52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# data\n",
    "X = (np.concatenate((np.random.uniform(low=-2.0, high=0.4, size=50), np.random.uniform(low=-0.4, high=3.0, size=50)))).reshape(-1,1)\n",
    "Y = np.array([0]*50 + [1]*50).reshape(-1,1)\n",
    "\n",
    "# fit the perceptron\n",
    "p=PerceptronLogistic([1],0.1,500,0.01)\n",
    "history=p.fit(X, Y)\n",
    "p.plot_history(history)\n",
    "Y_pred = p.predict(X)\n",
    "plt.show()\n",
    "\n",
    "# plotting data and prediction\n",
    "plt.scatter(X,Y_pred)\n",
    "plt.scatter(X,Y,c='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('class')\n",
    "plt.show()"
   ],
   "id": "7d7962c0e3454694"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
